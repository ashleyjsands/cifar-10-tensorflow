{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribution: Some of this code is from tensorflow tutorials and tensor udacity examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tarfile\n",
    "import ntpath\n",
    "import cPickle, gzip\n",
    "import os\n",
    "from six.moves import urllib\n",
    "import sys\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "('Successfully downloaded', 'cifar-10-python.tar.gz', 170498071, 'bytes.')\n",
      "Opening CIFAR 10 dataset\n",
      "Finished opening CIFAR 10 dataset\n",
      "Training set (40000, 3072) (40000,)\n",
      "Validation set (10000, 3072) (10000,)\n",
      "Test set (10000, 3072) (10000,)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \".\"\n",
    "DATA_URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "\n",
    "def maybe_download():\n",
    "    \"\"\"Download and extract the tarball from Alex's website.\"\"\"\n",
    "    dest_directory = data_dir\n",
    "    if not os.path.exists(dest_directory):\n",
    "        os.makedirs(dest_directory)\n",
    "    filename = DATA_URL.split('/')[-1]\n",
    "    filepath = os.path.join(dest_directory, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\r>> Downloading %s %.1f%%' % (filename,\n",
    "                float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "        filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, reporthook=_progress)\n",
    "    print()\n",
    "    statinfo = os.stat(filepath)\n",
    "    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "\n",
    "def load_cifar_10_dataset():\n",
    "    print \"Opening CIFAR 10 dataset\"\n",
    "    dataset = {}\n",
    "    with tarfile.open(data_dir + \"/cifar-10-python.tar.gz\", \"r:gz\") as tar:\n",
    "        for member in tar.getmembers():\n",
    "            if member.isfile():\n",
    "                if \"_batch\" in member.name:\n",
    "                    file_name = ntpath.basename(member.name)\n",
    "                    f = tar.extractfile(member)\n",
    "                    batch_dataset = cPickle.load(f) \n",
    "                    dataset[file_name] = batch_dataset\n",
    "                elif member.name.endswith(\"batches.meta\"):\n",
    "                    f = tar.extractfile(member)\n",
    "                    label_names = cPickle.load(f) \n",
    "                    dataset[\"meta\"] = label_names\n",
    "    print \"Finished opening CIFAR 10 dataset\"\n",
    "    return dataset\n",
    "\n",
    "def merge_datasets(dataset_one, dataset_two):\n",
    "    return {\n",
    "        \"data\": np.concatenate((dataset_one[\"data\"], dataset_two[\"data\"])),\n",
    "        \"labels\": dataset_one[\"labels\"] + dataset_two[\"labels\"], \n",
    "    }\n",
    "\n",
    "def get_merged_training_datasets(dataset_batches_dict):\n",
    "    training_dataset_names = [ \"data_batch_1\", \"data_batch_2\", \"data_batch_3\", \"data_batch_4\" ]\n",
    "    training_datasets = map(lambda name: dataset_batches_dict[name], training_dataset_names)\n",
    "    training_dataset_and_labels = reduce(merge_datasets, training_datasets)\n",
    "    validation_dataset_and_labels = dataset_batches_dict[\"data_batch_5\"]\n",
    "    test_dataset_and_labels = dataset_batches_dict[\"test_batch\"]\n",
    "    return (\n",
    "        np.asarray(training_dataset_and_labels[\"data\"]), np.asarray(training_dataset_and_labels[\"labels\"]),\n",
    "        np.asarray(validation_dataset_and_labels[\"data\"]), np.asarray(validation_dataset_and_labels[\"labels\"]),\n",
    "        np.asarray(test_dataset_and_labels[\"data\"]), np.asarray(test_dataset_and_labels[\"labels\"])\n",
    "    )\n",
    "\n",
    "maybe_download()\n",
    "dataset_batches_dict = load_cifar_10_dataset()\n",
    "label_names = dataset_batches_dict[\"meta\"][\"label_names\"]\n",
    "train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels = get_merged_training_datasets(dataset_batches_dict)\n",
    "\n",
    "print 'Training set', train_dataset.shape, train_labels.shape\n",
    "print 'Validation set', valid_dataset.shape, valid_labels.shape\n",
    "print 'Test set', test_dataset.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (40000, 32, 32, 3) (40000, 10)\n",
      "Validation set (10000, 32, 32, 3) (10000, 10)\n",
      "Test set (10000, 32, 32, 3) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 32\n",
    "num_labels = 10\n",
    "num_channels = 3 # RGB\n",
    "\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    #dataset = dataset.reshape(\n",
    "    #  (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    \n",
    "    # the dataset is of a shape (*, num_channels * image_size * image_size) \n",
    "    # with the red values first, followed by the green, then blue.\n",
    "    dataset = dataset\n",
    "    x = dataset.reshape((-1, num_channels, image_size * image_size)) # break the channels into their own axes.\n",
    "    y = x.transpose([0, 2, 1]) # This transpose the matrix by swapping the second and third axes, but not the first. This puts matching RGB values together\n",
    "    reformated_dataset = y.reshape((-1, image_size, image_size, num_channels)).astype(np.float32) # Turn the dataset into a 4D tensor of a collection of images, with axes of width, height and colour channels.\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return reformated_dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print 'Training set', train_dataset.shape, train_labels.shape\n",
    "print 'Validation set', valid_dataset.shape, valid_labels.shape\n",
    "print 'Test set', test_dataset.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_best_step(steps_to_predictions, labels):\n",
    "    best_accuracy = 0\n",
    "    best_accuracy_step = 0\n",
    "    for step, predictions in steps_to_predictions.iteritems():\n",
    "        acc = accuracy(predictions, labels)\n",
    "        steps_to_predictions[step] = predictions\n",
    "        if acc > best_accuracy:\n",
    "            best_accuracy = acc\n",
    "            best_accuracy_step = step\n",
    "    return best_accuracy, best_accuracy_step\n",
    "\n",
    "def visualise_accuracies(steps_to_validation_predictions, steps_to_test_predictions):\n",
    "    best_validation_accuracy, best_validation_step = get_best_step(steps_to_validation_predictions, valid_labels)\n",
    "    print \"The best validation accuracy was %s at step %s\" % (best_validation_accuracy, best_validation_step)\n",
    "    best_test_accuracy, best_test_step = get_best_step(steps_to_test_predictions, test_labels)\n",
    "    print \"The best test accuracy was %s at step %s\" % (best_test_accuracy, best_test_step)\n",
    "    \n",
    "    best_prediction = steps_to_test_predictions[best_test_step]\n",
    "    correct_prediction_indexes = []\n",
    "    incorrect_prediction_indexes = []\n",
    "    index = 0\n",
    "    for accurate in np.argmax(best_prediction, 1) == np.argmax(test_labels, 1):\n",
    "        if accurate:\n",
    "            correct_prediction_indexes.append(index)\n",
    "        else:\n",
    "            incorrect_prediction_indexes.append(index)\n",
    "        index += 1\n",
    "    return correct_prediction_indexes, incorrect_prediction_indexes\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint, shuffle\n",
    "\n",
    "def get_index_of_one_hot_vector(one_hot_vector):\n",
    "    for i in range(len(one_hot_vector)):\n",
    "        if one_hot_vector[i] == 1.0:\n",
    "            return i\n",
    "    raise \"Not a one_hot_vector\"\n",
    "    \n",
    "def display_test_data(data_set, labels, data_index, figure, subplot_index, width=5, height=5):\n",
    "    a = figure.add_subplot(width, height, subplot_index)\n",
    "    data = data_set[data_index,:,:,:]\n",
    "    decimal_code_to_fraction_quotient = 255.0\n",
    "    reshaped_data = data.reshape((image_size, image_size,-1)).astype(np.float32) / decimal_code_to_fraction_quotient\n",
    "    plt.axis(\"off\")\n",
    "    #plt.figure(figsize=(100, 100))\n",
    "    plt.imshow(reshaped_data, cmap=plt.cm.hot)\n",
    "    label = get_index_of_one_hot_vector(labels[data_index])\n",
    "    a.set_title(label_names[label])\n",
    "    \n",
    "def display_random_data(data_set, labels, number_of_data=25):\n",
    "    figure_size = math.ceil(pow(number_of_data, 0.5))\n",
    "    figure = plt.figure()\n",
    "    for i in range(number_of_data):\n",
    "        data_index = randint(0, len(data_set) - 1)\n",
    "        display_test_data(data_set, labels, data_index, figure, i, width=figure_size, height=figure_size)\n",
    "\n",
    "    figure.subplots_adjust(hspace=1.5)\n",
    "\n",
    "def display_data(dataset, labels, indexes, number_of_data=25):\n",
    "    figure_size = math.ceil(pow(number_of_data, 0.5))\n",
    "    figure = plt.figure()\n",
    "    for i in range(number_of_data):\n",
    "        display_test_data(dataset, labels, indexes[i], figure, i, width=figure_size, height=figure_size)\n",
    "\n",
    "    figure.subplots_adjust(hspace=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT0AAAEKCAYAAABg5IKIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBgYGBgYGBgYGBgb+/+V/AaTkUavzZI7QAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10da10cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_size = 9\n",
    "index_start = 45\n",
    "display_data(train_dataset, train_labels, range(index_start, index_start + display_size), number_of_data=display_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "class Model():\n",
    "    \n",
    "    def __init__(self, graph, batch_size, tf_train_dataset, tf_train_labels, tf_valid_dataset, tf_test_dataset, dropout_keep_probability, logits, loss, optimizer, train_prediction, valid_prediction, test_prediction):\n",
    "        self.graph = graph\n",
    "        self.batch_size = batch_size\n",
    "        self.tf_train_dataset = tf_train_dataset\n",
    "        self.tf_train_labels = tf_train_labels\n",
    "        self.tf_valid_dataset = tf_valid_dataset\n",
    "        self.tf_test_dataset = tf_test_dataset\n",
    "        self.dropout_keep_probability = dropout_keep_probability\n",
    "        self.logits = logits\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.train_prediction = train_prediction\n",
    "        self.valid_prediction = valid_prediction\n",
    "        self.test_prediction = test_prediction\n",
    "        \n",
    "def get_l2_loss(l2_lambda, layer_weights):\n",
    "    return l2_lambda * sum(map(tf.nn.l2_loss, layer_weights))\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "def create_same_padding_3_conv_one_hidden_model(learning_rate = 0.05, initialised_weights_stddev = 0.1, feature_maps = 16, number_of_hidden_neurons = 64, batch_size = 32, l2_lambda = 0.1, decay_steps = 10000, decay_rate = 0.96):\n",
    "    patch_size = 5\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "        # Input data.\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "        dropout_keep_probability = tf.placeholder(tf.float32)\n",
    "        \n",
    "        # Variables\n",
    "        layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, num_channels, feature_maps], stddev=initialised_weights_stddev))\n",
    "        layer1_biases = tf.Variable(tf.zeros([feature_maps]))\n",
    "\n",
    "        layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, feature_maps, feature_maps], stddev=initialised_weights_stddev))\n",
    "        layer2_biases = tf.Variable(tf.constant(initialised_weights_stddev * 10, shape=[feature_maps]))\n",
    "\n",
    "\n",
    "        conv_layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, feature_maps, feature_maps], stddev=initialised_weights_stddev))\n",
    "        conv_layer3_biases = tf.Variable(tf.constant(initialised_weights_stddev * 10, shape=[feature_maps]))\n",
    "\n",
    "        #layel3_weights = tf.Variable(tf.truncated_normal(\n",
    "        #    [image_size / 4 * image_size / 4 * feature_maps, number_of_hidden_neurons], stddev=initialised_weights_stddev))\n",
    "        number_of_conv_layers = 3\n",
    "        layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "            [int(math.ceil(image_size / (2.0 ** number_of_conv_layers)) * math.ceil(image_size / (2.0 ** number_of_conv_layers)) * feature_maps), number_of_hidden_neurons], stddev=initialised_weights_stddev))\n",
    "        layer3_biases = tf.Variable(tf.constant(initialised_weights_stddev * 10, shape=[number_of_hidden_neurons]))\n",
    "\n",
    "\n",
    "        layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "            [number_of_hidden_neurons, num_labels], stddev=initialised_weights_stddev))\n",
    "        layer4_biases = tf.Variable(tf.constant(initialised_weights_stddev * 10, shape=[num_labels]))\n",
    "        \n",
    "        \n",
    "\n",
    "        # Model.\n",
    "        def create_model_graph(data, add_dropout = False):\n",
    "            conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            relu = tf.nn.relu(conv + layer1_biases)\n",
    "            hidden = tf.nn.max_pool(relu, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "            conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            relu = tf.nn.relu(conv + layer2_biases)\n",
    "            hidden = tf.nn.max_pool(relu, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "            conv = tf.nn.conv2d(hidden, conv_layer3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            relu = tf.nn.relu(conv + conv_layer3_biases)\n",
    "            hidden = tf.nn.max_pool(relu, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "            shape = hidden.get_shape().as_list()\n",
    "            reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "            if add_dropout:\n",
    "                hidden = tf.nn.dropout(hidden, dropout_keep_probability)\n",
    "            return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "        # Training computation.\n",
    "        logits = create_model_graph(tf_train_dataset, add_dropout = True)\n",
    "        layer_weights = [layer1_weights, layer2_weights, conv_layer3_weights, layer3_weights, layer4_weights]\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) + get_l2_loss(l2_lambda, layer_weights))\n",
    "\n",
    "        # Optimizer.\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        decayed_learning_rate = tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(decayed_learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "        # Predictions for the training, validation, and test data.\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        valid_prediction = tf.nn.softmax(create_model_graph(tf_valid_dataset))\n",
    "        test_prediction = tf.nn.softmax(create_model_graph(tf_test_dataset))\n",
    "        \n",
    "        return Model(graph, batch_size, tf_train_dataset, tf_train_labels, tf_valid_dataset, tf_test_dataset, dropout_keep_probability, logits, loss, optimizer, train_prediction, valid_prediction, test_prediction)\n",
    "\n",
    "def create_cv_cv_mp_cv_cv_mp_one_hidden_model(learning_rate = 0.05, initialised_weights_stddev = 0.1, feature_maps = 16, number_of_hidden_neurons = 64, batch_size = 32, l2_lambda = 0.1, decay_steps = 10000, decay_rate = 0.96):\n",
    "    patch_size = 5\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "        # Input data.\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "        dropout_keep_probability = tf.placeholder(tf.float32)\n",
    "        \n",
    "        # Variables\n",
    "        layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, num_channels, feature_maps], stddev=initialised_weights_stddev))\n",
    "        layer1_biases = tf.Variable(tf.zeros([feature_maps]))\n",
    "\n",
    "        layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, feature_maps, feature_maps], stddev=initialised_weights_stddev))\n",
    "        layer2_biases = tf.Variable(tf.constant(initialised_weights_stddev * 10, shape=[feature_maps]))\n",
    "\n",
    "\n",
    "        conv_layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, feature_maps, feature_maps], stddev=initialised_weights_stddev))\n",
    "        conv_layer3_biases = tf.Variable(tf.constant(initialised_weights_stddev * 10, shape=[feature_maps]))\n",
    "        \n",
    "        conv_layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, feature_maps, feature_maps], stddev=initialised_weights_stddev))\n",
    "        conv_layer4_biases = tf.Variable(tf.constant(initialised_weights_stddev * 10, shape=[feature_maps]))\n",
    "\n",
    "        number_of_max_pool_layers = 2\n",
    "        conv_output_size = int(math.ceil(image_size / (2.0 ** number_of_max_pool_layers)) * math.ceil(image_size / (2.0 ** number_of_max_pool_layers)) * feature_maps)\n",
    "        #print \"conv_output_size %s\" % conv_output_size\n",
    "        layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "            [conv_output_size, number_of_hidden_neurons], stddev=initialised_weights_stddev))\n",
    "        layer3_biases = tf.Variable(tf.constant(initialised_weights_stddev * 10, shape=[number_of_hidden_neurons]))\n",
    "\n",
    "\n",
    "        layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "            [number_of_hidden_neurons, num_labels], stddev=initialised_weights_stddev))\n",
    "        layer4_biases = tf.Variable(tf.constant(initialised_weights_stddev * 10, shape=[num_labels]))\n",
    "\n",
    "        # Model.\n",
    "        def create_model_graph(data, add_dropout = False):\n",
    "            conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + layer1_biases)\n",
    "            shape = hidden.get_shape().as_list()\n",
    "            #print \"hidden shape: %s\" % shape\n",
    "\n",
    "            conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            relu = tf.nn.relu(conv + layer2_biases)\n",
    "            hidden = tf.nn.max_pool(relu, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "            shape = hidden.get_shape().as_list()\n",
    "            #print \"hidden shape: %s\" % shape\n",
    "\n",
    "            conv = tf.nn.conv2d(hidden, conv_layer3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + conv_layer3_biases)\n",
    "            \n",
    "            conv = tf.nn.conv2d(hidden, conv_layer4_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            relu = tf.nn.relu(conv + conv_layer4_biases)\n",
    "            hidden = tf.nn.max_pool(relu, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "            shape = hidden.get_shape().as_list()\n",
    "            #print \"hidden shape: %s\" % shape\n",
    "\n",
    "            shape = hidden.get_shape().as_list()\n",
    "            #print \"hidden shape: %s\" % shape\n",
    "            reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "            if add_dropout:\n",
    "                hidden = tf.nn.dropout(hidden, dropout_keep_probability)\n",
    "            return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "        # Training computation.\n",
    "        logits = create_model_graph(tf_train_dataset, add_dropout = True)\n",
    "        layer_weights = [layer1_weights, layer2_weights, conv_layer3_weights, conv_layer4_weights, layer3_weights, layer4_weights]\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) + get_l2_loss(l2_lambda, layer_weights))\n",
    "\n",
    "        # Optimizer.\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        decayed_learning_rate = tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(decayed_learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "        # Predictions for the training, validation, and test data.\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        valid_prediction = tf.nn.softmax(create_model_graph(tf_valid_dataset))\n",
    "        test_prediction = tf.nn.softmax(create_model_graph(tf_test_dataset))\n",
    "        \n",
    "        return Model(graph, batch_size, tf_train_dataset, tf_train_labels, tf_valid_dataset, tf_test_dataset, dropout_keep_probability, logits, loss, optimizer, train_prediction, valid_prediction, test_prediction)\n",
    "    \n",
    "\n",
    "def create_three_double_conv_layers_one_hidden_model(learning_rate = 0.05, initialised_weights_stddev = 0.1, feature_maps = 16, number_of_hidden_neurons = 64, batch_size = 32, l2_lambda = 0.1, decay_steps = 10000, decay_rate = 0.96):\n",
    "    patch_size = 5\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "        # Input data.\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "        dropout_keep_probability = tf.placeholder(tf.float32)\n",
    "        \n",
    "        # Variables\n",
    "        layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, num_channels, feature_maps], stddev=initialised_weights_stddev))\n",
    "        layer1_biases = tf.Variable(tf.zeros([feature_maps]))\n",
    "\n",
    "        layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, feature_maps, feature_maps], stddev=initialised_weights_stddev))\n",
    "        layer2_biases = tf.Variable(tf.constant(initialised_weights_stddev * 10, shape=[feature_maps]))\n",
    "\n",
    "        conv_layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, feature_maps, feature_maps], stddev=initialised_weights_stddev))\n",
    "        conv_layer3_biases = tf.Variable(tf.constant(initialised_weights_stddev * 10, shape=[feature_maps]))\n",
    "        \n",
    "        conv_layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, feature_maps, feature_maps], stddev=initialised_weights_stddev))\n",
    "        conv_layer4_biases = tf.Variable(tf.constant(initialised_weights_stddev * 10, shape=[feature_maps]))\n",
    "        \n",
    "        conv_layer5_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, feature_maps, feature_maps], stddev=initialised_weights_stddev))\n",
    "        conv_layer5_biases = tf.Variable(tf.constant(initialised_weights_stddev * 10, shape=[feature_maps]))\n",
    "        \n",
    "        conv_layer6_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, feature_maps, feature_maps], stddev=initialised_weights_stddev))\n",
    "        conv_layer6_biases = tf.Variable(tf.constant(initialised_weights_stddev * 10, shape=[feature_maps]))\n",
    "\n",
    "        number_of_max_pool_layers = 3\n",
    "        conv_output_size = int(math.ceil(image_size / (2.0 ** number_of_max_pool_layers)) * math.ceil(image_size / (2.0 ** number_of_max_pool_layers)) * feature_maps)\n",
    "        #print \"conv_output_size %s\" % conv_output_size\n",
    "        layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "            [conv_output_size, number_of_hidden_neurons], stddev=initialised_weights_stddev))\n",
    "        layer3_biases = tf.Variable(tf.constant(initialised_weights_stddev * 10, shape=[number_of_hidden_neurons]))\n",
    "\n",
    "\n",
    "        layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "            [number_of_hidden_neurons, num_labels], stddev=initialised_weights_stddev))\n",
    "        layer4_biases = tf.Variable(tf.constant(initialised_weights_stddev * 10, shape=[num_labels]))\n",
    "\n",
    "        # Model.\n",
    "        def create_model_graph(data, add_dropout = False):\n",
    "            conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + layer1_biases)\n",
    "            #shape = hidden.get_shape().as_list()\n",
    "            #print \"hidden shape: %s\" % shape\n",
    "\n",
    "            conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            relu = tf.nn.relu(conv + layer2_biases)\n",
    "            hidden = tf.nn.max_pool(relu, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "            #shape = hidden.get_shape().as_list()\n",
    "            #print \"hidden shape: %s\" % shape\n",
    "\n",
    "            conv = tf.nn.conv2d(hidden, conv_layer3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + conv_layer3_biases)\n",
    "            \n",
    "            conv = tf.nn.conv2d(hidden, conv_layer4_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            relu = tf.nn.relu(conv + conv_layer4_biases)\n",
    "            hidden = tf.nn.max_pool(relu, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "            #shape = hidden.get_shape().as_list()\n",
    "            #print \"hidden shape: %s\" % shape\n",
    "            \n",
    "            conv = tf.nn.conv2d(hidden, conv_layer5_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + conv_layer5_biases)\n",
    "            \n",
    "            conv = tf.nn.conv2d(hidden, conv_layer6_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            relu = tf.nn.relu(conv + conv_layer6_biases)\n",
    "            hidden = tf.nn.max_pool(relu, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "            #shape = hidden.get_shape().as_list()\n",
    "            #print \"hidden shape: %s\" % shape\n",
    "\n",
    "            shape = hidden.get_shape().as_list()\n",
    "            #print \"hidden shape: %s\" % shape\n",
    "            reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "            if add_dropout:\n",
    "                hidden = tf.nn.dropout(hidden, dropout_keep_probability)\n",
    "            return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "        # Training computation.\n",
    "        logits = create_model_graph(tf_train_dataset, add_dropout = True)\n",
    "        layer_weights = [layer1_weights, layer2_weights, conv_layer3_weights, conv_layer4_weights, conv_layer5_weights, conv_layer6_weights, layer3_weights, layer4_weights]\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) + get_l2_loss(l2_lambda, layer_weights))\n",
    "\n",
    "        # Optimizer.\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        decayed_learning_rate = tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(decayed_learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "        # Predictions for the training, validation, and test data.\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        valid_prediction = tf.nn.softmax(create_model_graph(tf_valid_dataset))\n",
    "        test_prediction = tf.nn.softmax(create_model_graph(tf_test_dataset))\n",
    "        \n",
    "        return Model(graph, batch_size, tf_train_dataset, tf_train_labels, tf_valid_dataset, tf_test_dataset, dropout_keep_probability, logits, loss, optimizer, train_prediction, valid_prediction, test_prediction)    \n",
    "\n",
    "def train_model(model, steps, dropout_keep_prob):\n",
    "    batch_size = model.batch_size\n",
    "    start_time = time.time()\n",
    "    steps_to_validation_predictions = {}\n",
    "    steps_to_test_predictions = {}\n",
    "    with tf.Session(graph=model.graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print \"Initialized\"\n",
    "        for step in xrange(steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            feed_dict = {model.tf_train_dataset : batch_data, model.tf_train_labels : batch_labels, model.dropout_keep_probability: dropout_keep_prob}\n",
    "            _, l, predictions = session.run(\n",
    "                [model.optimizer, model.loss, model.train_prediction], feed_dict=feed_dict)\n",
    "            if (step % 1000 == 0):\n",
    "                print \"Minibatch loss at step\", step, \":\", l\n",
    "                print \"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)\n",
    "                validation_predictions = model.valid_prediction.eval()\n",
    "                steps_to_validation_predictions[step] = validation_predictions\n",
    "                print \"Validation accuracy at step %s: %.1f%%\" % (step, accuracy(validation_predictions, valid_labels))\n",
    "            if (step != 0 and step % 5000 == 0 and step != (steps - 1)):\n",
    "                test_predictions = model.test_prediction.eval()\n",
    "                steps_to_test_predictions[step] =  test_predictions\n",
    "                print \"Test accuracy at step %s: %.1f%%\" % (step, accuracy(test_predictions, test_labels))\n",
    "        test_predictions = model.test_prediction.eval()\n",
    "        steps_to_test_predictions[step] = test_predictions\n",
    "        print \"Test accuracy at step %s: %.1f%%\\n\" % (step, accuracy(test_predictions, test_labels))\n",
    "        seconds_in_an_hour = 60 * 60\n",
    "        print \"Elapsed time: %s hours\" % ((time.time() - start_time) / seconds_in_an_hour)\n",
    "        return steps_to_validation_predictions, steps_to_test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My goal is to achieve 75.86% test accuracy** or higher for the CIFAR-10 dataset. My stretch goal is 90% test accuracy.\n",
    "\n",
    "My first naive adoption of the Not-MNIST conv net achieved ~26% after 5000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 243.877\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy at step 0: 11.4%\n",
      "Minibatch loss at step 1000 : 12.3505\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy at step 1000: 13.4%\n",
      "Minibatch loss at step 2000 : 10.5204\n",
      "Minibatch accuracy: 21.9%\n",
      "Validation accuracy at step 2000: 13.6%\n",
      "Minibatch loss at step 3000 : 9.01231\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy at step 3000: 17.7%\n",
      "Minibatch loss at step 4000 : 7.80978\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy at step 4000: 18.6%\n",
      "Minibatch loss at step 5000 : 6.81654\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy at step 5000: 19.4%\n",
      "Test accuracy at step 5000: 19.3%\n",
      "\n",
      "Elapsed time: 0.109593903621 hours\n",
      "The best validation accuracy was 19.36 at step 5000\n",
      "The best test accuracy was 19.29 at step 5000\n"
     ]
    }
   ],
   "source": [
    "model = create_same_padding_3_conv_one_hidden_model(learning_rate = 0.001, l2_lambda = 0.1, feature_maps = 16, number_of_hidden_neurons = 64, decay_steps = 10000, decay_rate = 0.96)\n",
    "steps_to_validation_predictions, steps_to_test_predictions = train_model(model, 5001, dropout_keep_prob = 0.9)\n",
    "correct_prediction_indexes, incorrect_prediction_indexes = visualise_accuracies(steps_to_validation_predictions, steps_to_test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the maxpool from same padding to valid padding improved performance slightly for 17K steps but performed worse at step 80K with a test accuracy of 61.5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a convnet model with params: {'number_of_hidden_neurons': 64, 'decay_steps': 10000, 'dropout_keep_prob': 0.9, 'feature_maps': 16, 'decay_rate': 0.96, 'initialised_weights_stddev': 0.1, 'steps': 85000, 'l2_lambda': 0.1, 'learning_rate': 0.001}\n",
      "Start time 13:38\n",
      "Initialized\n",
      "Minibatch loss at step 0 : 176.443\n",
      "Minibatch accuracy: 3.1%\n",
      "Validation accuracy at step 0: 10.2%\n",
      "Minibatch loss at step 1000 : 12.2727\n",
      "Minibatch accuracy: 3.1%\n",
      "Validation accuracy at step 1000: 12.5%\n",
      "Minibatch loss at step 2000 : 10.3679\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy at step 2000: 13.9%\n",
      "Minibatch loss at step 3000 : 8.94248\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy at step 3000: 18.2%\n",
      "Minibatch loss at step 4000 : 7.46955\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy at step 4000: 20.4%\n",
      "Minibatch loss at step 5000 : 6.21282\n",
      "Minibatch accuracy: 31.2%\n",
      "Validation accuracy at step 5000: 30.1%\n",
      "Test accuracy at step 5000: 31.1%\n",
      "Minibatch loss at step 6000 : 5.48967\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy at step 6000: 35.2%\n",
      "Minibatch loss at step 7000 : 4.74502\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy at step 7000: 38.9%\n",
      "Minibatch loss at step 8000 : 4.17235\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 8000: 37.6%\n",
      "Minibatch loss at step 9000 : 3.64391\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 9000: 42.3%\n",
      "Minibatch loss at step 10000 : 3.39299\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 10000: 43.3%\n",
      "Test accuracy at step 10000: 43.3%\n",
      "Minibatch loss at step 11000 : 3.01777\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 11000: 42.4%\n",
      "Minibatch loss at step 12000 : 2.8549\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 12000: 46.0%\n",
      "Minibatch loss at step 13000 : 2.44859\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 13000: 47.4%\n",
      "Minibatch loss at step 14000 : 2.52267\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy at step 14000: 47.8%\n",
      "Minibatch loss at step 15000 : 2.43381\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy at step 15000: 48.9%\n",
      "Test accuracy at step 15000: 48.9%\n",
      "Minibatch loss at step 16000 : 2.26011\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy at step 16000: 49.8%\n",
      "Minibatch loss at step 17000 : 1.68219\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 17000: 50.0%\n",
      "Minibatch loss at step 18000 : 1.61051\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 18000: 50.8%\n",
      "Minibatch loss at step 19000 : 1.96831\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy at step 19000: 52.2%\n",
      "Minibatch loss at step 20000 : 1.86614\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 20000: 53.2%\n",
      "Test accuracy at step 20000: 52.9%\n",
      "Minibatch loss at step 21000 : 1.68983\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 21000: 48.6%\n",
      "Minibatch loss at step 22000 : 1.90163\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 22000: 51.2%\n",
      "Minibatch loss at step 23000 : 1.77398\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 23000: 54.4%\n",
      "Minibatch loss at step 24000 : 1.60581\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 24000: 55.2%\n",
      "Minibatch loss at step 25000 : 1.62209\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 25000: 55.0%\n",
      "Test accuracy at step 25000: 54.6%\n",
      "Minibatch loss at step 26000 : 1.67151\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 26000: 54.2%\n",
      "Minibatch loss at step 27000 : 1.26015\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 27000: 56.4%\n",
      "Minibatch loss at step 28000 : 1.53424\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 28000: 54.2%\n",
      "Minibatch loss at step 29000 : 1.80078\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 29000: 55.8%\n",
      "Minibatch loss at step 30000 : 1.51448\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 30000: 56.7%\n",
      "Test accuracy at step 30000: 56.7%\n",
      "Minibatch loss at step 31000 : 1.51178\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 31000: 58.2%\n",
      "Minibatch loss at step 32000 : 1.75007\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 32000: 53.3%\n",
      "Minibatch loss at step 33000 : 1.21029\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 33000: 57.6%\n",
      "Minibatch loss at step 34000 : 1.45593\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 34000: 58.8%\n",
      "Minibatch loss at step 35000 : 1.34723\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 35000: 59.2%\n",
      "Test accuracy at step 35000: 58.8%\n",
      "Minibatch loss at step 36000 : 1.86388\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 36000: 58.2%\n",
      "Minibatch loss at step 37000 : 1.35656\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 37000: 58.6%\n",
      "Minibatch loss at step 38000 : 1.70634\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 38000: 59.0%\n",
      "Minibatch loss at step 39000 : 1.92376\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 39000: 59.4%\n",
      "Minibatch loss at step 40000 : 1.47006\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 40000: 57.6%\n",
      "Test accuracy at step 40000: 58.1%\n",
      "Minibatch loss at step 41000 : 1.7685\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 41000: 58.4%\n",
      "Minibatch loss at step 42000 : 1.17992\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 42000: 58.7%\n",
      "Minibatch loss at step 43000 : 1.26281\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 43000: 60.7%\n",
      "Minibatch loss at step 44000 : 1.54271\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 44000: 59.0%\n",
      "Minibatch loss at step 45000 : 1.32637\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 45000: 60.3%\n",
      "Test accuracy at step 45000: 58.7%\n",
      "Minibatch loss at step 46000 : 1.32247\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 46000: 61.8%\n",
      "Minibatch loss at step 47000 : 1.67889\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 47000: 59.2%\n",
      "Minibatch loss at step 48000 : 1.20934\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 48000: 59.9%\n",
      "Minibatch loss at step 49000 : 1.25316\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 49000: 60.6%\n",
      "Minibatch loss at step 50000 : 1.49756\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 50000: 60.2%\n",
      "Test accuracy at step 50000: 59.8%\n",
      "Minibatch loss at step 51000 : 1.38218\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 51000: 61.5%\n",
      "Minibatch loss at step 52000 : 1.45442\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 52000: 60.4%\n",
      "Minibatch loss at step 53000 : 1.235\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 53000: 62.3%\n",
      "Minibatch loss at step 54000 : 1.43604\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 54000: 62.6%\n",
      "Minibatch loss at step 55000 : 1.47096\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 55000: 63.0%\n",
      "Test accuracy at step 55000: 61.9%\n",
      "Minibatch loss at step 56000 : 1.68062\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 56000: 63.0%\n",
      "Minibatch loss at step 57000 : 1.49935\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 57000: 63.8%\n",
      "Minibatch loss at step 58000 : 1.55595\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 58000: 63.7%\n",
      "Minibatch loss at step 59000 : 1.17275\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 59000: 61.1%\n",
      "Minibatch loss at step 60000 : 1.4201\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 60000: 62.5%\n",
      "Test accuracy at step 60000: 61.9%\n",
      "Minibatch loss at step 61000 : 1.66286\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 61000: 61.4%\n",
      "Minibatch loss at step 62000 : 1.26613\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 62000: 63.5%\n",
      "Minibatch loss at step 63000 : 1.21746\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 63000: 62.2%\n",
      "Minibatch loss at step 64000 : 1.37484\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 64000: 64.2%\n",
      "Minibatch loss at step 65000 : 1.36379\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 65000: 60.7%\n",
      "Test accuracy at step 65000: 60.4%\n",
      "Minibatch loss at step 66000 : 1.64253\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy at step 66000: 60.2%\n",
      "Minibatch loss at step 67000 : 1.42653\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 67000: 57.7%\n",
      "Minibatch loss at step 68000 : 1.40429\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 68000: 60.7%\n",
      "Minibatch loss at step 69000 : 1.46217\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 69000: 60.6%\n",
      "Minibatch loss at step 70000 : 1.48803\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 70000: 64.1%\n",
      "Test accuracy at step 70000: 63.6%\n",
      "Minibatch loss at step 71000 : 1.12798\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 71000: 63.1%\n",
      "Minibatch loss at step 72000 : 1.17461\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 72000: 65.2%\n",
      "Minibatch loss at step 73000 : 1.3509\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 73000: 64.1%\n",
      "Minibatch loss at step 74000 : 1.26442\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 74000: 65.6%\n",
      "Minibatch loss at step 75000 : 1.98029\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 75000: 64.0%\n",
      "Test accuracy at step 75000: 63.0%\n",
      "Minibatch loss at step 76000 : 1.32842\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 76000: 65.1%\n",
      "Minibatch loss at step 77000 : 1.26882\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 77000: 65.3%\n",
      "Minibatch loss at step 78000 : 1.48001\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 78000: 62.0%\n",
      "Minibatch loss at step 79000 : 1.41734\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 79000: 65.3%\n",
      "Minibatch loss at step 80000 : 1.22491\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 80000: 65.6%\n",
      "Test accuracy at step 80000: 64.5%\n",
      "Minibatch loss at step 81000 : 1.38641\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 81000: 63.6%\n",
      "Minibatch loss at step 82000 : 1.16786\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 82000: 66.2%\n",
      "Minibatch loss at step 83000 : 1.41061\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 83000: 65.2%\n",
      "Minibatch loss at step 84000 : 1.41978\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 84000: 63.5%\n",
      "Test accuracy at step 84999: 65.8%\n",
      "\n",
      "End time 15:23\n",
      "The best validation accuracy was 66.2 at step 82000\n",
      "The best test accuracy was 65.82 at step 84999\n"
     ]
    }
   ],
   "source": [
    "# Remove the fix for the image display and left all inputs to be values between 0 and 255\n",
    "# Running with the best previous model hyperparameters\n",
    "results = train_bigger_cnn(85000, learning_rate = 0.001, l2_lambda = 0.1, feature_maps = 16, dropout_keep_prob = 0.9, number_of_hidden_neurons = 64, decay_steps = 10000, decay_rate = 0.96)\n",
    "steps_to_validation_predictions, steps_to_test_predictions = results\n",
    "correct_prediction_indexes, incorrect_prediction_indexes = visualise_accuracies(steps_to_validation_predictions, steps_to_test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the image display fix and rerunning for 85K steps go a validation accuracy of 66.2% and a **test accuracy of 65.82%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing dropout keep probablity to 0.5 and 0.1 decreased test accuracy dramatically.\n",
    "\n",
    "Changing the dropout keep probability to 1.0 slightly decreased test accuracy.\n",
    "\n",
    "Doubling the hidden layer to 128 made the model perform worse and nearly doubled the time to train.\n",
    "\n",
    "Doubling the number of feature maps slightly decreased performance.\n",
    "\n",
    "Changing the L2 lambda decreased performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a convnet model with params: {'number_of_hidden_neurons': 64, 'decay_steps': 10000, 'dropout_keep_prob': 0.9, 'feature_maps': 16, 'decay_rate': 0.96, 'initialised_weights_stddev': 0.1, 'steps': 250001, 'l2_lambda': 0.1, 'learning_rate': 0.001}\n",
      "Start time 22:02\n",
      "Initialized\n",
      "Minibatch loss at step 0 : 123.544\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy at step 0: 10.2%\n",
      "Minibatch loss at step 1000 : 11.717\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy at step 1000: 28.6%\n",
      "Minibatch loss at step 2000 : 9.77351\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy at step 2000: 31.8%\n",
      "Minibatch loss at step 3000 : 8.34768\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy at step 3000: 35.3%\n",
      "Minibatch loss at step 4000 : 7.11417\n",
      "Minibatch accuracy: 34.4%\n",
      "Validation accuracy at step 4000: 35.1%\n",
      "Minibatch loss at step 5000 : 6.27981\n",
      "Minibatch accuracy: 28.1%\n",
      "Validation accuracy at step 5000: 38.2%\n",
      "Test accuracy at step 5000: 37.7%\n",
      "Minibatch loss at step 6000 : 5.45688\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy at step 6000: 39.3%\n",
      "Minibatch loss at step 7000 : 4.76187\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy at step 7000: 41.2%\n",
      "Minibatch loss at step 8000 : 4.32785\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy at step 8000: 40.4%\n",
      "Minibatch loss at step 9000 : 3.68055\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 9000: 42.9%\n",
      "Minibatch loss at step 10000 : 3.3426\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 10000: 43.8%\n",
      "Test accuracy at step 10000: 44.6%\n",
      "Minibatch loss at step 11000 : 3.2267\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy at step 11000: 42.4%\n",
      "Minibatch loss at step 12000 : 2.89007\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 12000: 45.2%\n",
      "Minibatch loss at step 13000 : 2.55872\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 13000: 47.1%\n",
      "Minibatch loss at step 14000 : 2.46697\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy at step 14000: 48.0%\n",
      "Minibatch loss at step 15000 : 2.57458\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 15000: 48.3%\n",
      "Test accuracy at step 15000: 47.9%\n",
      "Minibatch loss at step 16000 : 2.20257\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy at step 16000: 50.8%\n",
      "Minibatch loss at step 17000 : 1.97244\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 17000: 50.4%\n",
      "Minibatch loss at step 18000 : 1.67102\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 18000: 51.3%\n",
      "Minibatch loss at step 19000 : 2.07737\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy at step 19000: 52.3%\n",
      "Minibatch loss at step 20000 : 1.89437\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy at step 20000: 52.1%\n",
      "Test accuracy at step 20000: 51.7%\n",
      "Minibatch loss at step 21000 : 1.81701\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 21000: 48.4%\n",
      "Minibatch loss at step 22000 : 1.89232\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 22000: 51.9%\n",
      "Minibatch loss at step 23000 : 1.75594\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 23000: 54.1%\n",
      "Minibatch loss at step 24000 : 1.68647\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 24000: 54.5%\n",
      "Minibatch loss at step 25000 : 1.54741\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 25000: 54.1%\n",
      "Test accuracy at step 25000: 53.7%\n",
      "Minibatch loss at step 26000 : 1.63215\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 26000: 53.6%\n",
      "Minibatch loss at step 27000 : 1.39306\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 27000: 55.1%\n",
      "Minibatch loss at step 28000 : 1.62665\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 28000: 54.9%\n",
      "Minibatch loss at step 29000 : 1.62574\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 29000: 56.8%\n",
      "Minibatch loss at step 30000 : 1.72009\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 30000: 57.1%\n",
      "Test accuracy at step 30000: 56.6%\n",
      "Minibatch loss at step 31000 : 1.60091\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 31000: 56.6%\n",
      "Minibatch loss at step 32000 : 1.73065\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 32000: 55.0%\n",
      "Minibatch loss at step 33000 : 1.27931\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 33000: 56.0%\n",
      "Minibatch loss at step 34000 : 1.24411\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 34000: 57.7%\n",
      "Minibatch loss at step 35000 : 1.25268\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 35000: 57.9%\n",
      "Test accuracy at step 35000: 57.6%\n",
      "Minibatch loss at step 36000 : 1.78421\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 36000: 58.4%\n",
      "Minibatch loss at step 37000 : 1.39994\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 37000: 57.4%\n",
      "Minibatch loss at step 38000 : 1.73584\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 38000: 58.9%\n",
      "Minibatch loss at step 39000 : 1.89723\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 39000: 59.0%\n",
      "Minibatch loss at step 40000 : 1.57875\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 40000: 53.9%\n",
      "Test accuracy at step 40000: 54.7%\n",
      "Minibatch loss at step 41000 : 1.8842\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 41000: 59.1%\n",
      "Minibatch loss at step 42000 : 1.35124\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 42000: 59.2%\n",
      "Minibatch loss at step 43000 : 1.33768\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 43000: 59.5%\n",
      "Minibatch loss at step 44000 : 1.57761\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 44000: 57.6%\n",
      "Minibatch loss at step 45000 : 1.33903\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 45000: 59.5%\n",
      "Test accuracy at step 45000: 58.9%\n",
      "Minibatch loss at step 46000 : 1.37666\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 46000: 60.8%\n",
      "Minibatch loss at step 47000 : 1.68265\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 47000: 58.3%\n",
      "Minibatch loss at step 48000 : 1.19882\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 48000: 59.4%\n",
      "Minibatch loss at step 49000 : 1.08634\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 49000: 61.2%\n",
      "Minibatch loss at step 50000 : 1.6989\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 50000: 60.6%\n",
      "Test accuracy at step 50000: 60.4%\n",
      "Minibatch loss at step 51000 : 1.49789\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 51000: 59.7%\n",
      "Minibatch loss at step 52000 : 1.4635\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 52000: 60.2%\n",
      "Minibatch loss at step 53000 : 1.33823\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 53000: 61.1%\n",
      "Minibatch loss at step 54000 : 1.43412\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 54000: 62.4%\n",
      "Minibatch loss at step 55000 : 1.44907\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 55000: 62.2%\n",
      "Test accuracy at step 55000: 62.0%\n",
      "Minibatch loss at step 56000 : 1.64101\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 56000: 62.8%\n",
      "Minibatch loss at step 57000 : 1.30749\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 57000: 62.0%\n",
      "Minibatch loss at step 58000 : 1.64773\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 58000: 62.9%\n",
      "Minibatch loss at step 59000 : 1.06517\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 59000: 60.9%\n",
      "Minibatch loss at step 60000 : 1.502\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 60000: 60.5%\n",
      "Test accuracy at step 60000: 60.9%\n",
      "Minibatch loss at step 61000 : 1.66417\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 61000: 60.4%\n",
      "Minibatch loss at step 62000 : 1.43047\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 62000: 62.3%\n",
      "Minibatch loss at step 63000 : 1.26768\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 63000: 62.8%\n",
      "Minibatch loss at step 64000 : 1.49341\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 64000: 63.7%\n",
      "Minibatch loss at step 65000 : 1.40745\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 65000: 62.7%\n",
      "Test accuracy at step 65000: 63.2%\n",
      "Minibatch loss at step 66000 : 1.62654\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 66000: 55.3%\n",
      "Minibatch loss at step 67000 : 1.37868\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 67000: 58.3%\n",
      "Minibatch loss at step 68000 : 1.48276\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 68000: 57.6%\n",
      "Minibatch loss at step 69000 : 1.43359\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 69000: 61.1%\n",
      "Minibatch loss at step 70000 : 1.29389\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 70000: 63.9%\n",
      "Test accuracy at step 70000: 63.6%\n",
      "Minibatch loss at step 71000 : 1.26498\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 71000: 63.5%\n",
      "Minibatch loss at step 72000 : 1.20398\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 72000: 63.0%\n",
      "Minibatch loss at step 73000 : 1.32895\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 73000: 64.7%\n",
      "Minibatch loss at step 74000 : 1.32887\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 74000: 65.0%\n",
      "Minibatch loss at step 75000 : 1.83341\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 75000: 63.5%\n",
      "Test accuracy at step 75000: 63.5%\n",
      "Minibatch loss at step 76000 : 1.37623\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 76000: 65.4%\n",
      "Minibatch loss at step 77000 : 1.27188\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 77000: 63.4%\n",
      "Minibatch loss at step 78000 : 1.31487\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 78000: 63.1%\n",
      "Minibatch loss at step 79000 : 1.42141\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 79000: 65.5%\n",
      "Minibatch loss at step 80000 : 1.31619\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 80000: 65.0%\n",
      "Test accuracy at step 80000: 64.9%\n",
      "Minibatch loss at step 81000 : 1.3653\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 81000: 63.0%\n",
      "Minibatch loss at step 82000 : 1.15571\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 82000: 64.5%\n",
      "Minibatch loss at step 83000 : 1.30524\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 83000: 64.4%\n",
      "Minibatch loss at step 84000 : 1.4455\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 84000: 64.2%\n",
      "Minibatch loss at step 85000 : 1.15214\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 85000: 64.9%\n",
      "Test accuracy at step 85000: 65.5%\n",
      "Minibatch loss at step 86000 : 1.13962\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy at step 86000: 65.2%\n",
      "Minibatch loss at step 87000 : 1.30035\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 87000: 62.7%\n",
      "Minibatch loss at step 88000 : 1.2525\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 88000: 64.1%\n",
      "Minibatch loss at step 89000 : 1.46124\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 89000: 65.2%\n",
      "Minibatch loss at step 90000 : 1.24311\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 90000: 65.3%\n",
      "Test accuracy at step 90000: 65.5%\n",
      "Minibatch loss at step 91000 : 1.6101\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 91000: 65.2%\n",
      "Minibatch loss at step 92000 : 1.09354\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy at step 92000: 65.7%\n",
      "Minibatch loss at step 93000 : 1.48206\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 93000: 63.3%\n",
      "Minibatch loss at step 94000 : 1.39448\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 94000: 65.8%\n",
      "Minibatch loss at step 95000 : 1.45261\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 95000: 65.7%\n",
      "Test accuracy at step 95000: 65.3%\n",
      "Minibatch loss at step 96000 : 1.36828\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 96000: 66.0%\n",
      "Minibatch loss at step 97000 : 1.20788\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 97000: 65.3%\n",
      "Minibatch loss at step 98000 : 1.33146\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 98000: 65.8%\n",
      "Minibatch loss at step 99000 : 1.00244\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy at step 99000: 65.1%\n",
      "Minibatch loss at step 100000 : 1.04518\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy at step 100000: 65.6%\n",
      "Test accuracy at step 100000: 65.3%\n",
      "Minibatch loss at step 101000 : 1.18747\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 101000: 65.8%\n",
      "Minibatch loss at step 102000 : 1.18951\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 102000: 64.6%\n",
      "Minibatch loss at step 103000 : 1.29647\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 103000: 66.9%\n",
      "Minibatch loss at step 104000 : 1.29131\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 104000: 65.6%\n",
      "Minibatch loss at step 105000 : 1.29803\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 105000: 63.7%\n",
      "Test accuracy at step 105000: 64.1%\n",
      "Minibatch loss at step 106000 : 1.27463\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 106000: 66.1%\n",
      "Minibatch loss at step 107000 : 1.33423\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 107000: 67.0%\n",
      "Minibatch loss at step 108000 : 1.19107\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 108000: 66.3%\n",
      "Minibatch loss at step 109000 : 1.41266\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 109000: 66.6%\n",
      "Minibatch loss at step 110000 : 1.11316\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 110000: 66.1%\n",
      "Test accuracy at step 110000: 66.4%\n",
      "Minibatch loss at step 111000 : 1.07289\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 111000: 66.0%\n",
      "Minibatch loss at step 112000 : 1.14501\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 112000: 65.5%\n",
      "Minibatch loss at step 113000 : 1.38348\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 113000: 64.0%\n",
      "Minibatch loss at step 114000 : 1.62577\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 114000: 65.9%\n",
      "Minibatch loss at step 115000 : 1.66424\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 115000: 63.9%\n",
      "Test accuracy at step 115000: 64.3%\n",
      "Minibatch loss at step 116000 : 1.51503\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 116000: 66.7%\n",
      "Minibatch loss at step 117000 : 1.38614\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 117000: 66.3%\n",
      "Minibatch loss at step 118000 : 1.06446\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy at step 118000: 66.1%\n",
      "Minibatch loss at step 119000 : 1.12645\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 119000: 66.4%\n",
      "Minibatch loss at step 120000 : 1.07712\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 120000: 64.8%\n",
      "Test accuracy at step 120000: 65.4%\n",
      "Minibatch loss at step 121000 : 1.35074\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 121000: 66.8%\n",
      "Minibatch loss at step 122000 : 1.23675\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 122000: 66.0%\n",
      "Minibatch loss at step 123000 : 1.30048\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 123000: 66.4%\n",
      "Minibatch loss at step 124000 : 1.27209\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 124000: 66.8%\n",
      "Minibatch loss at step 125000 : 1.30539\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 125000: 67.2%\n",
      "Test accuracy at step 125000: 67.0%\n",
      "Minibatch loss at step 126000 : 1.40585\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 126000: 66.9%\n",
      "Minibatch loss at step 127000 : 1.41143\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 127000: 67.0%\n",
      "Minibatch loss at step 128000 : 1.32149\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 128000: 65.7%\n",
      "Minibatch loss at step 129000 : 1.23201\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 129000: 66.3%\n",
      "Minibatch loss at step 130000 : 1.88288\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 130000: 65.6%\n",
      "Test accuracy at step 130000: 66.2%\n",
      "Minibatch loss at step 131000 : 1.39118\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 131000: 63.8%\n",
      "Minibatch loss at step 132000 : 0.857603\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy at step 132000: 66.9%\n",
      "Minibatch loss at step 133000 : 1.11293\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 133000: 66.5%\n",
      "Minibatch loss at step 134000 : 1.56374\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 134000: 65.2%\n",
      "Minibatch loss at step 135000 : 1.33404\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 135000: 66.6%\n",
      "Test accuracy at step 135000: 67.2%\n",
      "Minibatch loss at step 136000 : 1.33877\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 136000: 67.3%\n",
      "Minibatch loss at step 137000 : 1.39675\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 137000: 67.2%\n",
      "Minibatch loss at step 138000 : 1.63642\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 138000: 67.3%\n",
      "Minibatch loss at step 139000 : 1.39529\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 139000: 65.6%\n",
      "Minibatch loss at step 140000 : 1.27632\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 140000: 66.6%\n",
      "Test accuracy at step 140000: 66.7%\n",
      "Minibatch loss at step 141000 : 1.36198\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 141000: 66.0%\n",
      "Minibatch loss at step 142000 : 1.03807\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy at step 142000: 66.8%\n",
      "Minibatch loss at step 143000 : 1.32911\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 143000: 66.8%\n",
      "Minibatch loss at step 144000 : 1.26092\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 144000: 64.4%\n",
      "Minibatch loss at step 145000 : 0.927163\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy at step 145000: 64.4%\n",
      "Test accuracy at step 145000: 64.3%\n",
      "Minibatch loss at step 146000 : 1.21594\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 146000: 67.1%\n",
      "Minibatch loss at step 147000 : 1.23294\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 147000: 67.3%\n",
      "Minibatch loss at step 148000 : 1.14622\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 148000: 67.2%\n",
      "Minibatch loss at step 149000 : 1.33097\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 149000: 62.0%\n",
      "Minibatch loss at step 150000 : 1.06068\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 150000: 67.3%\n",
      "Test accuracy at step 150000: 67.5%\n",
      "Minibatch loss at step 151000 : 1.14849\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 151000: 63.8%\n",
      "Minibatch loss at step 152000 : 1.54546\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 152000: 65.2%\n",
      "Minibatch loss at step 153000 : 1.49133\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 153000: 67.5%\n",
      "Minibatch loss at step 154000 : 1.21472\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 154000: 66.4%\n",
      "Minibatch loss at step 155000 : 1.37425\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 155000: 67.0%\n",
      "Test accuracy at step 155000: 66.6%\n",
      "Minibatch loss at step 156000 : 1.1765\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 156000: 67.2%\n",
      "Minibatch loss at step 157000 : 1.1791\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 157000: 66.9%\n",
      "Minibatch loss at step 158000 : 1.08138\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy at step 158000: 67.3%\n",
      "Minibatch loss at step 159000 : 1.29328\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 159000: 67.1%\n",
      "Minibatch loss at step 160000 : 1.27804\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 160000: 67.8%\n",
      "Test accuracy at step 160000: 67.3%\n",
      "Minibatch loss at step 161000 : 1.29257\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 161000: 67.2%\n",
      "Minibatch loss at step 162000 : 1.38483\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 162000: 66.7%\n",
      "Minibatch loss at step 163000 : 1.24947\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 163000: 67.5%\n",
      "Minibatch loss at step 164000 : 1.32586\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 164000: 65.6%\n",
      "Minibatch loss at step 165000 : 1.18061\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 165000: 66.1%\n",
      "Test accuracy at step 165000: 66.2%\n",
      "Minibatch loss at step 166000 : 1.13497\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 166000: 66.1%\n",
      "Minibatch loss at step 167000 : 1.22789\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 167000: 67.5%\n",
      "Minibatch loss at step 168000 : 1.24324\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 168000: 67.3%\n",
      "Minibatch loss at step 169000 : 1.55823\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 169000: 67.6%\n",
      "Minibatch loss at step 170000 : 1.08121\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy at step 170000: 66.4%\n",
      "Test accuracy at step 170000: 66.7%\n",
      "Minibatch loss at step 171000 : 1.21178\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 171000: 67.4%\n",
      "Minibatch loss at step 172000 : 1.16357\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 172000: 67.7%\n",
      "Minibatch loss at step 173000 : 1.22253\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 173000: 66.4%\n",
      "Minibatch loss at step 174000 : 1.17005\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 174000: 67.8%\n",
      "Minibatch loss at step 175000 : 1.58764\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 175000: 67.5%\n",
      "Test accuracy at step 175000: 67.0%\n",
      "Minibatch loss at step 176000 : 1.10879\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 176000: 67.3%\n",
      "Minibatch loss at step 177000 : 1.3952\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 177000: 67.4%\n",
      "Minibatch loss at step 178000 : 1.61099\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 178000: 66.8%\n",
      "Minibatch loss at step 179000 : 1.4207\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 179000: 66.9%\n",
      "Minibatch loss at step 180000 : 1.19953\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 180000: 67.5%\n",
      "Test accuracy at step 180000: 67.1%\n",
      "Minibatch loss at step 181000 : 1.28794\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 181000: 65.1%\n",
      "Minibatch loss at step 182000 : 1.10285\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 182000: 67.4%\n",
      "Minibatch loss at step 183000 : 1.35893\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 183000: 66.4%\n",
      "Minibatch loss at step 184000 : 1.40247\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 184000: 66.6%\n",
      "Minibatch loss at step 185000 : 1.18379\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 185000: 67.4%\n",
      "Test accuracy at step 185000: 67.7%\n",
      "Minibatch loss at step 186000 : 1.23703\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 186000: 66.0%\n",
      "Minibatch loss at step 187000 : 1.21463\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 187000: 67.0%\n",
      "Minibatch loss at step 188000 : 1.31183\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 188000: 67.6%\n",
      "Minibatch loss at step 189000 : 1.08481\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy at step 189000: 68.2%\n",
      "Minibatch loss at step 190000 : 1.05115\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 190000: 66.9%\n",
      "Test accuracy at step 190000: 66.8%\n",
      "Minibatch loss at step 191000 : 1.84213\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 191000: 64.3%\n",
      "Minibatch loss at step 192000 : 1.01603\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy at step 192000: 67.0%\n",
      "Minibatch loss at step 193000 : 1.27675\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 193000: 66.7%\n",
      "Minibatch loss at step 194000 : 1.38171\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 194000: 68.3%\n",
      "Minibatch loss at step 195000 : 1.43048\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 195000: 67.6%\n",
      "Test accuracy at step 195000: 67.7%\n",
      "Minibatch loss at step 196000 : 1.23275\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 196000: 65.3%\n",
      "Minibatch loss at step 197000 : 1.21746\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 197000: 67.2%\n",
      "Minibatch loss at step 198000 : 1.14182\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 198000: 66.7%\n",
      "Minibatch loss at step 199000 : 1.10831\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 199000: 67.6%\n",
      "Minibatch loss at step 200000 : 1.46653\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 200000: 67.2%\n",
      "Test accuracy at step 200000: 67.3%\n",
      "Minibatch loss at step 201000 : 1.54794\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 201000: 68.2%\n",
      "Minibatch loss at step 202000 : 1.62594\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 202000: 66.8%\n",
      "Minibatch loss at step 203000 : 1.24622\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 203000: 67.3%\n",
      "Minibatch loss at step 204000 : 0.954767\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy at step 204000: 68.3%\n",
      "Minibatch loss at step 205000 : 1.38197\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 205000: 65.8%\n",
      "Test accuracy at step 205000: 65.5%\n",
      "Minibatch loss at step 206000 : 1.30387\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 206000: 67.2%\n",
      "Minibatch loss at step 207000 : 1.29822\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 207000: 68.0%\n",
      "Minibatch loss at step 208000 : 1.26187\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 208000: 66.3%\n",
      "Minibatch loss at step 209000 : 1.03632\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 209000: 67.4%\n",
      "Minibatch loss at step 210000 : 1.45114\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 210000: 66.4%\n",
      "Test accuracy at step 210000: 66.8%\n",
      "Minibatch loss at step 211000 : 1.16257\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 211000: 67.7%\n",
      "Minibatch loss at step 212000 : 1.12928\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 212000: 66.9%\n",
      "Minibatch loss at step 213000 : 1.33\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 213000: 67.9%\n",
      "Minibatch loss at step 214000 : 1.26676\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 214000: 66.8%\n",
      "Minibatch loss at step 215000 : 1.10198\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 215000: 67.7%\n",
      "Test accuracy at step 215000: 68.3%\n",
      "Minibatch loss at step 216000 : 1.40102\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 216000: 67.3%\n",
      "Minibatch loss at step 217000 : 1.20176\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 217000: 67.8%\n",
      "Minibatch loss at step 218000 : 1.12726\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 218000: 67.1%\n",
      "Minibatch loss at step 219000 : 1.05036\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 219000: 68.2%\n",
      "Minibatch loss at step 220000 : 1.56486\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 220000: 68.0%\n",
      "Test accuracy at step 220000: 68.0%\n",
      "Minibatch loss at step 221000 : 1.59228\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 221000: 68.0%\n",
      "Minibatch loss at step 222000 : 1.2427\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy at step 222000: 66.4%\n",
      "Minibatch loss at step 223000 : 1.20841\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 223000: 67.8%\n",
      "Minibatch loss at step 224000 : 1.68845\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 224000: 68.0%\n",
      "Minibatch loss at step 225000 : 1.07681\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 225000: 67.0%\n",
      "Test accuracy at step 225000: 66.8%\n",
      "Minibatch loss at step 226000 : 1.50971\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 226000: 67.3%\n",
      "Minibatch loss at step 227000 : 1.21559\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 227000: 67.7%\n",
      "Minibatch loss at step 228000 : 1.3703\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 228000: 68.1%\n",
      "Minibatch loss at step 229000 : 1.25096\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 229000: 68.5%\n",
      "Minibatch loss at step 230000 : 1.08011\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 230000: 67.3%\n",
      "Test accuracy at step 230000: 67.6%\n",
      "Minibatch loss at step 231000 : 1.40866\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 231000: 66.3%\n",
      "Minibatch loss at step 232000 : 1.24137\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 232000: 66.6%\n",
      "Minibatch loss at step 233000 : 1.35004\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 233000: 67.8%\n",
      "Minibatch loss at step 234000 : 1.33061\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 234000: 67.7%\n",
      "Minibatch loss at step 235000 : 1.09149\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 235000: 67.4%\n",
      "Test accuracy at step 235000: 67.1%\n",
      "Minibatch loss at step 236000 : 1.3127\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 236000: 66.4%\n",
      "Minibatch loss at step 237000 : 1.36142\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 237000: 67.3%\n",
      "Minibatch loss at step 238000 : 1.31396\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 238000: 66.4%\n",
      "Minibatch loss at step 239000 : 1.29898\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 239000: 68.3%\n",
      "Minibatch loss at step 240000 : 0.950359\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 240000: 66.8%\n",
      "Test accuracy at step 240000: 66.5%\n",
      "Minibatch loss at step 241000 : 1.18445\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy at step 241000: 67.5%\n",
      "Minibatch loss at step 242000 : 1.35815\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 242000: 67.2%\n",
      "Minibatch loss at step 243000 : 1.29228\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 243000: 68.1%\n",
      "Minibatch loss at step 244000 : 1.34373\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 244000: 68.0%\n",
      "Minibatch loss at step 245000 : 1.05269\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 245000: 66.4%\n",
      "Test accuracy at step 245000: 66.1%\n",
      "Minibatch loss at step 246000 : 1.39447\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 246000: 66.7%\n",
      "Minibatch loss at step 247000 : 1.23993\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 247000: 68.1%\n",
      "Minibatch loss at step 248000 : 1.3401\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 248000: 67.6%\n",
      "Minibatch loss at step 249000 : 1.15368\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 249000: 68.0%\n",
      "Minibatch loss at step 250000 : 1.40101\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 250000: 68.8%\n",
      "Test accuracy at step 250000: 68.2%\n",
      "\n",
      "End time 03:07\n",
      "The best validation accuracy was 68.77 at step 250000\n",
      "The best test accuracy was 68.3 at step 215000\n"
     ]
    }
   ],
   "source": [
    "# Best model to date with more steps\n",
    "results = train_bigger_cnn(250001, learning_rate = 0.001, l2_lambda = 0.1, feature_maps = 16, dropout_keep_prob = 0.9, number_of_hidden_neurons = 64, decay_steps = 10000, decay_rate = 0.96)\n",
    "steps_to_validation_predictions, steps_to_test_predictions = results\n",
    "correct_prediction_indexes, incorrect_prediction_indexes = visualise_accuracies(steps_to_validation_predictions, steps_to_test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of steps to 250K **marginally** improved the performance to: **68.77% for validation accuracy and 68.3% for test accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a convnet model with params: {'number_of_hidden_neurons': 64, 'decay_steps': 5000, 'dropout_keep_prob': 0.9, 'feature_maps': 16, 'decay_rate': 0.96, 'initialised_weights_stddev': 0.1, 'steps': 250001, 'l2_lambda': 0.1, 'learning_rate': 0.001}\n",
      "Start time 17:23\n",
      "Initialized\n",
      "Minibatch loss at step 0 : 114.16\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy at step 0: 10.2%\n",
      "Minibatch loss at step 1000 : 12.6392\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy at step 1000: 20.9%\n",
      "Minibatch loss at step 2000 : 10.8846\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy at step 2000: 22.8%\n",
      "Minibatch loss at step 3000 : 9.26286\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy at step 3000: 25.5%\n",
      "Minibatch loss at step 4000 : 8.02656\n",
      "Minibatch accuracy: 21.9%\n",
      "Validation accuracy at step 4000: 29.7%\n",
      "Minibatch loss at step 5000 : 6.88922\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy at step 5000: 28.6%\n",
      "Test accuracy at step 5000: 29.9%\n",
      "Minibatch loss at step 6000 : 5.84557\n",
      "Minibatch accuracy: 21.9%\n",
      "Validation accuracy at step 6000: 36.4%\n",
      "Minibatch loss at step 7000 : 4.96011\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 7000: 38.8%\n",
      "Minibatch loss at step 8000 : 4.69648\n",
      "Minibatch accuracy: 21.9%\n",
      "Validation accuracy at step 8000: 40.3%\n",
      "Minibatch loss at step 9000 : 3.83584\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 9000: 41.3%\n",
      "Minibatch loss at step 10000 : 3.7023\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy at step 10000: 37.8%\n",
      "Test accuracy at step 10000: 38.7%\n",
      "Minibatch loss at step 11000 : 3.29536\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy at step 11000: 41.7%\n",
      "Minibatch loss at step 12000 : 3.12396\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 12000: 43.9%\n",
      "Minibatch loss at step 13000 : 2.75749\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 13000: 46.9%\n",
      "Minibatch loss at step 14000 : 2.54\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy at step 14000: 47.4%\n",
      "Minibatch loss at step 15000 : 2.43653\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 15000: 48.3%\n",
      "Test accuracy at step 15000: 48.5%\n",
      "Minibatch loss at step 16000 : 2.4398\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy at step 16000: 50.3%\n",
      "Minibatch loss at step 17000 : 1.96684\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 17000: 50.6%\n",
      "Minibatch loss at step 18000 : 1.85556\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 18000: 51.0%\n",
      "Minibatch loss at step 19000 : 2.10481\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 19000: 51.5%\n",
      "Minibatch loss at step 20000 : 1.89188\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 20000: 53.1%\n",
      "Test accuracy at step 20000: 52.8%\n",
      "Minibatch loss at step 21000 : 1.77441\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 21000: 50.8%\n",
      "Minibatch loss at step 22000 : 1.86105\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 22000: 52.1%\n",
      "Minibatch loss at step 23000 : 1.96179\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 23000: 55.0%\n",
      "Minibatch loss at step 24000 : 1.7534\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 24000: 55.8%\n",
      "Minibatch loss at step 25000 : 1.47491\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 25000: 54.6%\n",
      "Test accuracy at step 25000: 54.2%\n",
      "Minibatch loss at step 26000 : 1.5795\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 26000: 54.0%\n",
      "Minibatch loss at step 27000 : 1.36304\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 27000: 56.0%\n",
      "Minibatch loss at step 28000 : 1.58326\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 28000: 55.8%\n",
      "Minibatch loss at step 29000 : 1.60535\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 29000: 57.6%\n",
      "Minibatch loss at step 30000 : 1.52929\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 30000: 57.6%\n",
      "Test accuracy at step 30000: 57.2%\n",
      "Minibatch loss at step 31000 : 1.53666\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 31000: 58.6%\n",
      "Minibatch loss at step 32000 : 1.64001\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 32000: 56.1%\n",
      "Minibatch loss at step 33000 : 1.29061\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 33000: 58.1%\n",
      "Minibatch loss at step 34000 : 1.47795\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 34000: 58.2%\n",
      "Minibatch loss at step 35000 : 1.32175\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 35000: 58.0%\n",
      "Test accuracy at step 35000: 57.1%\n",
      "Minibatch loss at step 36000 : 1.58056\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 36000: 59.0%\n",
      "Minibatch loss at step 37000 : 1.36962\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 37000: 59.2%\n",
      "Minibatch loss at step 38000 : 1.70526\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 38000: 60.6%\n",
      "Minibatch loss at step 39000 : 1.81691\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 39000: 61.0%\n",
      "Minibatch loss at step 40000 : 1.46605\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 40000: 60.7%\n",
      "Test accuracy at step 40000: 60.1%\n",
      "Minibatch loss at step 41000 : 1.77489\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy at step 41000: 59.5%\n",
      "Minibatch loss at step 42000 : 1.24094\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 42000: 59.8%\n",
      "Minibatch loss at step 43000 : 1.25283\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 43000: 61.9%\n",
      "Minibatch loss at step 44000 : 1.5285\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 44000: 60.7%\n",
      "Minibatch loss at step 45000 : 1.2939\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 45000: 59.8%\n",
      "Test accuracy at step 45000: 59.8%\n",
      "Minibatch loss at step 46000 : 1.37331\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 46000: 62.8%\n",
      "Minibatch loss at step 47000 : 1.55959\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 47000: 58.4%\n",
      "Minibatch loss at step 48000 : 1.28477\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 48000: 61.3%\n",
      "Minibatch loss at step 49000 : 1.10598\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 49000: 62.8%\n",
      "Minibatch loss at step 50000 : 1.67942\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 50000: 61.4%\n",
      "Test accuracy at step 50000: 60.7%\n",
      "Minibatch loss at step 51000 : 1.51372\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 51000: 62.0%\n",
      "Minibatch loss at step 52000 : 1.3964\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 52000: 63.1%\n",
      "Minibatch loss at step 53000 : 1.16644\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 53000: 63.0%\n",
      "Minibatch loss at step 54000 : 1.28453\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 54000: 64.2%\n",
      "Minibatch loss at step 55000 : 1.49429\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 55000: 62.1%\n",
      "Test accuracy at step 55000: 62.1%\n",
      "Minibatch loss at step 56000 : 1.70522\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 56000: 64.5%\n",
      "Minibatch loss at step 57000 : 1.41848\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 57000: 64.0%\n",
      "Minibatch loss at step 58000 : 1.52766\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 58000: 62.7%\n",
      "Minibatch loss at step 59000 : 1.11193\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 59000: 62.5%\n",
      "Minibatch loss at step 60000 : 1.36464\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 60000: 61.6%\n",
      "Test accuracy at step 60000: 61.6%\n",
      "Minibatch loss at step 61000 : 1.61155\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 61000: 63.0%\n",
      "Minibatch loss at step 62000 : 1.35063\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 62000: 63.6%\n",
      "Minibatch loss at step 63000 : 1.14203\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 63000: 64.5%\n",
      "Minibatch loss at step 64000 : 1.35856\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 64000: 64.9%\n",
      "Minibatch loss at step 65000 : 1.3729\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 65000: 63.8%\n",
      "Test accuracy at step 65000: 63.1%\n",
      "Minibatch loss at step 66000 : 1.6948\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy at step 66000: 61.2%\n",
      "Minibatch loss at step 67000 : 1.4557\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 67000: 59.6%\n",
      "Minibatch loss at step 68000 : 1.35107\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 68000: 63.0%\n",
      "Minibatch loss at step 69000 : 1.49218\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 69000: 61.7%\n",
      "Minibatch loss at step 70000 : 1.36662\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 70000: 65.0%\n",
      "Test accuracy at step 70000: 64.7%\n",
      "Minibatch loss at step 71000 : 1.30113\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 71000: 63.8%\n",
      "Minibatch loss at step 72000 : 1.22818\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 72000: 65.0%\n",
      "Minibatch loss at step 73000 : 1.19706\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 73000: 64.8%\n",
      "Minibatch loss at step 74000 : 1.27652\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 74000: 65.9%\n",
      "Minibatch loss at step 75000 : 1.78676\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 75000: 64.2%\n",
      "Test accuracy at step 75000: 63.8%\n",
      "Minibatch loss at step 76000 : 1.26855\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 76000: 65.2%\n",
      "Minibatch loss at step 77000 : 1.35123\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 77000: 64.6%\n",
      "Minibatch loss at step 78000 : 1.58079\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 78000: 64.0%\n",
      "Minibatch loss at step 79000 : 1.45691\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 79000: 66.5%\n",
      "Minibatch loss at step 80000 : 1.30038\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 80000: 65.0%\n",
      "Test accuracy at step 80000: 64.7%\n",
      "Minibatch loss at step 81000 : 1.29592\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 81000: 64.1%\n",
      "Minibatch loss at step 82000 : 1.05953\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 82000: 66.2%\n",
      "Minibatch loss at step 83000 : 1.29944\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 83000: 65.1%\n",
      "Minibatch loss at step 84000 : 1.33514\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 84000: 65.8%\n",
      "Minibatch loss at step 85000 : 1.07413\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 85000: 66.0%\n",
      "Test accuracy at step 85000: 65.3%\n",
      "Minibatch loss at step 86000 : 1.27125\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 86000: 63.8%\n",
      "Minibatch loss at step 87000 : 1.36149\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 87000: 63.8%\n",
      "Minibatch loss at step 88000 : 1.16898\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 88000: 64.2%\n",
      "Minibatch loss at step 89000 : 1.31268\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 89000: 66.8%\n",
      "Minibatch loss at step 90000 : 1.2617\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 90000: 65.5%\n",
      "Test accuracy at step 90000: 65.0%\n",
      "Minibatch loss at step 91000 : 1.42713\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 91000: 66.3%\n",
      "Minibatch loss at step 92000 : 1.00452\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 92000: 66.0%\n",
      "Minibatch loss at step 93000 : 1.61543\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 93000: 65.9%\n",
      "Minibatch loss at step 94000 : 1.19599\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 94000: 65.9%\n",
      "Minibatch loss at step 95000 : 1.28496\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 95000: 66.3%\n",
      "Test accuracy at step 95000: 65.3%\n",
      "Minibatch loss at step 96000 : 1.22228\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 96000: 67.2%\n",
      "Minibatch loss at step 97000 : 1.21974\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 97000: 66.0%\n",
      "Minibatch loss at step 98000 : 1.20393\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 98000: 66.0%\n",
      "Minibatch loss at step 99000 : 0.921165\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy at step 99000: 65.1%\n",
      "Minibatch loss at step 100000 : 1.13177\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 100000: 66.4%\n",
      "Test accuracy at step 100000: 66.2%\n",
      "Minibatch loss at step 101000 : 1.36709\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 101000: 65.2%\n",
      "Minibatch loss at step 102000 : 1.16177\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 102000: 66.4%\n",
      "Minibatch loss at step 103000 : 1.26097\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 103000: 67.5%\n",
      "Minibatch loss at step 104000 : 1.48114\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 104000: 66.8%\n",
      "Minibatch loss at step 105000 : 1.50351\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 105000: 66.2%\n",
      "Test accuracy at step 105000: 65.6%\n",
      "Minibatch loss at step 106000 : 1.28065\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 106000: 66.2%\n",
      "Minibatch loss at step 107000 : 1.24816\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 107000: 66.7%\n",
      "Minibatch loss at step 108000 : 1.20063\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 108000: 67.0%\n",
      "Minibatch loss at step 109000 : 1.22714\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 109000: 66.3%\n",
      "Minibatch loss at step 110000 : 1.12747\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 110000: 66.8%\n",
      "Test accuracy at step 110000: 66.4%\n",
      "Minibatch loss at step 111000 : 1.12058\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 111000: 66.0%\n",
      "Minibatch loss at step 112000 : 1.28195\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 112000: 66.0%\n",
      "Minibatch loss at step 113000 : 1.42243\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 113000: 66.9%\n",
      "Minibatch loss at step 114000 : 1.45008\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 114000: 66.7%\n",
      "Minibatch loss at step 115000 : 1.66925\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 115000: 66.2%\n",
      "Test accuracy at step 115000: 65.4%\n",
      "Minibatch loss at step 116000 : 1.5504\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 116000: 65.7%\n",
      "Minibatch loss at step 117000 : 1.23428\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 117000: 67.2%\n",
      "Minibatch loss at step 118000 : 1.13205\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 118000: 67.4%\n",
      "Minibatch loss at step 119000 : 1.1768\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 119000: 66.4%\n",
      "Minibatch loss at step 120000 : 1.18939\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 120000: 66.3%\n",
      "Test accuracy at step 120000: 65.8%\n",
      "Minibatch loss at step 121000 : 1.2334\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 121000: 67.8%\n",
      "Minibatch loss at step 122000 : 1.20313\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 122000: 67.5%\n",
      "Minibatch loss at step 123000 : 1.36145\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 123000: 67.7%\n",
      "Minibatch loss at step 124000 : 1.22884\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 124000: 66.6%\n",
      "Minibatch loss at step 125000 : 1.33577\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 125000: 68.1%\n",
      "Test accuracy at step 125000: 67.5%\n",
      "Minibatch loss at step 126000 : 1.32085\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 126000: 68.3%\n",
      "Minibatch loss at step 127000 : 1.49208\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 127000: 67.2%\n",
      "Minibatch loss at step 128000 : 1.23369\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 128000: 65.8%\n",
      "Minibatch loss at step 129000 : 1.19818\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 129000: 68.3%\n",
      "Minibatch loss at step 130000 : 1.72497\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 130000: 66.3%\n",
      "Test accuracy at step 130000: 65.8%\n",
      "Minibatch loss at step 131000 : 1.3484\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 131000: 67.9%\n",
      "Minibatch loss at step 132000 : 0.88929\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy at step 132000: 66.8%\n",
      "Minibatch loss at step 133000 : 1.14211\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 133000: 67.1%\n",
      "Minibatch loss at step 134000 : 1.55826\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 134000: 65.9%\n",
      "Minibatch loss at step 135000 : 1.19964\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 135000: 67.5%\n",
      "Test accuracy at step 135000: 67.2%\n",
      "Minibatch loss at step 136000 : 1.37477\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 136000: 67.8%\n",
      "Minibatch loss at step 137000 : 1.46217\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 137000: 67.0%\n",
      "Minibatch loss at step 138000 : 1.6861\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 138000: 68.2%\n",
      "Minibatch loss at step 139000 : 1.41175\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 139000: 68.0%\n",
      "Minibatch loss at step 140000 : 1.23694\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 140000: 68.2%\n",
      "Test accuracy at step 140000: 67.3%\n",
      "Minibatch loss at step 141000 : 1.4687\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 141000: 66.9%\n",
      "Minibatch loss at step 142000 : 1.10523\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 142000: 67.4%\n",
      "Minibatch loss at step 143000 : 1.30925\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 143000: 67.3%\n",
      "Minibatch loss at step 144000 : 1.04107\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy at step 144000: 67.7%\n",
      "Minibatch loss at step 145000 : 0.916967\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy at step 145000: 66.6%\n",
      "Test accuracy at step 145000: 66.2%\n",
      "Minibatch loss at step 146000 : 1.25498\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 146000: 67.5%\n",
      "Minibatch loss at step 147000 : 1.18753\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 147000: 67.9%\n",
      "Minibatch loss at step 148000 : 1.09869\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 148000: 68.5%\n",
      "Minibatch loss at step 149000 : 1.28244\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 149000: 65.6%\n",
      "Minibatch loss at step 150000 : 1.05385\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 150000: 68.0%\n",
      "Test accuracy at step 150000: 67.3%\n",
      "Minibatch loss at step 151000 : 1.01951\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 151000: 66.8%\n",
      "Minibatch loss at step 152000 : 1.63273\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 152000: 67.4%\n",
      "Minibatch loss at step 153000 : 1.34026\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 153000: 68.2%\n",
      "Minibatch loss at step 154000 : 1.32408\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 154000: 67.1%\n",
      "Minibatch loss at step 155000 : 1.28468\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 155000: 67.5%\n",
      "Test accuracy at step 155000: 66.9%\n",
      "Minibatch loss at step 156000 : 1.33253\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 156000: 67.9%\n",
      "Minibatch loss at step 157000 : 1.23801\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 157000: 66.6%\n",
      "Minibatch loss at step 158000 : 1.0845\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy at step 158000: 68.5%\n",
      "Minibatch loss at step 159000 : 1.17062\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 159000: 67.6%\n",
      "Minibatch loss at step 160000 : 1.04112\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 160000: 68.6%\n",
      "Test accuracy at step 160000: 68.2%\n",
      "Minibatch loss at step 161000 : 1.32285\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 161000: 67.4%\n",
      "Minibatch loss at step 162000 : 1.27543\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 162000: 67.8%\n",
      "Minibatch loss at step 163000 : 1.23352\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 163000: 68.7%\n",
      "Minibatch loss at step 164000 : 1.26941\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 164000: 67.8%\n",
      "Minibatch loss at step 165000 : 1.1782\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy at step 165000: 67.2%\n",
      "Test accuracy at step 165000: 66.6%\n",
      "Minibatch loss at step 166000 : 1.19935\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 166000: 67.0%\n",
      "Minibatch loss at step 167000 : 1.14167\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 167000: 66.8%\n",
      "Minibatch loss at step 168000 : 1.33898\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 168000: 68.3%\n",
      "Minibatch loss at step 169000 : 1.37728\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 169000: 68.1%\n",
      "Minibatch loss at step 170000 : 1.05753\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 170000: 68.0%\n",
      "Test accuracy at step 170000: 67.4%\n",
      "Minibatch loss at step 171000 : 1.21555\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 171000: 68.4%\n",
      "Minibatch loss at step 172000 : 1.061\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 172000: 68.4%\n",
      "Minibatch loss at step 173000 : 1.18006\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 173000: 68.1%\n",
      "Minibatch loss at step 174000 : 1.11655\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 174000: 67.8%\n",
      "Minibatch loss at step 175000 : 1.50775\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 175000: 68.5%\n",
      "Test accuracy at step 175000: 68.0%\n",
      "Minibatch loss at step 176000 : 1.14116\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 176000: 68.0%\n",
      "Minibatch loss at step 177000 : 1.44457\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 177000: 67.7%\n",
      "Minibatch loss at step 178000 : 1.33718\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 178000: 67.9%\n",
      "Minibatch loss at step 179000 : 1.44139\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 179000: 67.8%\n",
      "Minibatch loss at step 180000 : 1.27865\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 180000: 68.0%\n",
      "Test accuracy at step 180000: 68.1%\n",
      "Minibatch loss at step 181000 : 1.19987\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 181000: 67.1%\n",
      "Minibatch loss at step 182000 : 1.1544\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 182000: 67.8%\n",
      "Minibatch loss at step 183000 : 1.41711\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 183000: 68.5%\n",
      "Minibatch loss at step 184000 : 1.36035\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 184000: 68.1%\n",
      "Minibatch loss at step 185000 : 1.24253\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 185000: 67.8%\n",
      "Test accuracy at step 185000: 67.8%\n",
      "Minibatch loss at step 186000 : 1.22216\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 186000: 67.2%\n",
      "Minibatch loss at step 187000 : 1.39851\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 187000: 68.1%\n",
      "Minibatch loss at step 188000 : 1.12093\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 188000: 67.8%\n",
      "Minibatch loss at step 189000 : 1.16887\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 189000: 68.7%\n",
      "Minibatch loss at step 190000 : 1.22388\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 190000: 68.5%\n",
      "Test accuracy at step 190000: 67.7%\n",
      "Minibatch loss at step 191000 : 1.50891\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 191000: 67.9%\n",
      "Minibatch loss at step 192000 : 0.976524\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy at step 192000: 68.6%\n",
      "Minibatch loss at step 193000 : 1.18274\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 193000: 68.2%\n",
      "Minibatch loss at step 194000 : 1.35123\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 194000: 68.6%\n",
      "Minibatch loss at step 195000 : 1.35638\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 195000: 68.2%\n",
      "Test accuracy at step 195000: 67.7%\n",
      "Minibatch loss at step 196000 : 1.12077\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 196000: 67.0%\n",
      "Minibatch loss at step 197000 : 1.15341\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 197000: 68.0%\n",
      "Minibatch loss at step 198000 : 1.14071\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy at step 198000: 67.9%\n",
      "Minibatch loss at step 199000 : 1.17833\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 199000: 68.2%\n",
      "Minibatch loss at step 200000 : 1.52132\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 200000: 68.1%\n",
      "Test accuracy at step 200000: 67.7%\n",
      "Minibatch loss at step 201000 : 1.29862\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 201000: 68.8%\n",
      "Minibatch loss at step 202000 : 1.39009\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 202000: 68.6%\n",
      "Minibatch loss at step 203000 : 1.26521\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 203000: 68.0%\n",
      "Minibatch loss at step 204000 : 1.12515\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 204000: 68.5%\n",
      "Minibatch loss at step 205000 : 1.39189\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 205000: 68.4%\n",
      "Test accuracy at step 205000: 67.5%\n",
      "Minibatch loss at step 206000 : 1.16262\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 206000: 68.3%\n",
      "Minibatch loss at step 207000 : 1.21606\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 207000: 68.7%\n",
      "Minibatch loss at step 208000 : 1.26798\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 208000: 68.3%\n",
      "Minibatch loss at step 209000 : 1.0118\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 209000: 67.2%\n",
      "Minibatch loss at step 210000 : 1.46174\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 210000: 68.2%\n",
      "Test accuracy at step 210000: 67.7%\n",
      "Minibatch loss at step 211000 : 1.10468\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy at step 211000: 68.0%\n",
      "Minibatch loss at step 212000 : 1.0241\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy at step 212000: 68.3%\n",
      "Minibatch loss at step 213000 : 1.28188\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 213000: 68.7%\n",
      "Minibatch loss at step 214000 : 1.21903\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 214000: 68.1%\n",
      "Minibatch loss at step 215000 : 1.14416\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 215000: 68.8%\n",
      "Test accuracy at step 215000: 68.2%\n",
      "Minibatch loss at step 216000 : 1.47062\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 216000: 68.4%\n",
      "Minibatch loss at step 217000 : 1.24992\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 217000: 68.0%\n",
      "Minibatch loss at step 218000 : 1.14854\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 218000: 68.6%\n",
      "Minibatch loss at step 219000 : 1.03656\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy at step 219000: 68.5%\n",
      "Minibatch loss at step 220000 : 1.38917\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 220000: 68.5%\n",
      "Test accuracy at step 220000: 68.2%\n",
      "Minibatch loss at step 221000 : 1.5451\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 221000: 68.2%\n",
      "Minibatch loss at step 222000 : 1.17013\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 222000: 68.3%\n",
      "Minibatch loss at step 223000 : 1.19018\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 223000: 68.9%\n",
      "Minibatch loss at step 224000 : 1.50131\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 224000: 68.6%\n",
      "Minibatch loss at step 225000 : 1.08529\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 225000: 68.2%\n",
      "Test accuracy at step 225000: 68.0%\n",
      "Minibatch loss at step 226000 : 1.59015\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 226000: 68.0%\n",
      "Minibatch loss at step 227000 : 1.25782\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 227000: 68.2%\n",
      "Minibatch loss at step 228000 : 1.28051\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 228000: 68.6%\n",
      "Minibatch loss at step 229000 : 1.15829\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 229000: 68.9%\n",
      "Minibatch loss at step 230000 : 1.07606\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 230000: 68.9%\n",
      "Test accuracy at step 230000: 68.5%\n",
      "Minibatch loss at step 231000 : 1.42668\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 231000: 67.8%\n",
      "Minibatch loss at step 232000 : 1.10263\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 232000: 67.8%\n",
      "Minibatch loss at step 233000 : 1.27417\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 233000: 68.7%\n",
      "Minibatch loss at step 234000 : 1.32842\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 234000: 68.5%\n",
      "Minibatch loss at step 235000 : 1.28956\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 235000: 68.4%\n",
      "Test accuracy at step 235000: 68.0%\n",
      "Minibatch loss at step 236000 : 1.44119\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 236000: 67.6%\n",
      "Minibatch loss at step 237000 : 1.28288\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 237000: 68.5%\n",
      "Minibatch loss at step 238000 : 1.1952\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 238000: 67.7%\n",
      "Minibatch loss at step 239000 : 1.47785\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 239000: 68.2%\n",
      "Minibatch loss at step 240000 : 0.945915\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy at step 240000: 68.3%\n",
      "Test accuracy at step 240000: 68.1%\n",
      "Minibatch loss at step 241000 : 1.17853\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 241000: 68.8%\n",
      "Minibatch loss at step 242000 : 1.1806\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 242000: 68.2%\n",
      "Minibatch loss at step 243000 : 1.18372\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 243000: 68.8%\n",
      "Minibatch loss at step 244000 : 1.27513\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 244000: 68.4%\n",
      "Minibatch loss at step 245000 : 1.151\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 245000: 67.1%\n",
      "Test accuracy at step 245000: 67.0%\n",
      "Minibatch loss at step 246000 : 1.3829\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 246000: 67.8%\n",
      "Minibatch loss at step 247000 : 1.23658\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 247000: 68.6%\n",
      "Minibatch loss at step 248000 : 1.10829\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 248000: 68.4%\n",
      "Minibatch loss at step 249000 : 1.10196\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 249000: 68.0%\n",
      "Minibatch loss at step 250000 : 1.33927\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 250000: 69.1%\n",
      "Test accuracy at step 250000: 68.8%\n",
      "\n",
      "End time 22:28\n",
      "The best validation accuracy was 69.08 at step 250000\n",
      "The best test accuracy was 68.77 at step 250000\n"
     ]
    }
   ],
   "source": [
    "# Halved the decay step\n",
    "results = train_bigger_cnn(250001, learning_rate = 0.001, l2_lambda = 0.1, feature_maps = 16, dropout_keep_prob = 0.9, number_of_hidden_neurons = 64, decay_steps = 5000, decay_rate = 0.96)\n",
    "steps_to_validation_predictions, steps_to_test_predictions = results\n",
    "correct_prediction_indexes, incorrect_prediction_indexes = visualise_accuracies(steps_to_validation_predictions, steps_to_test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Halving the decay step improved **validation accuracy to 69.08% and test accuracy to 68.77%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time 15:06\n",
      "Initialized\n",
      "Minibatch loss at step 0 : 212.079\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy at step 0: 10.8%\n",
      "Minibatch loss at step 1000 : 59.6236\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy at step 1000: 16.3%\n",
      "Minibatch loss at step 2000 : 49.3928\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy at step 2000: 17.9%\n",
      "Minibatch loss at step 3000 : 40.7772\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy at step 3000: 19.9%\n",
      "Minibatch loss at step 4000 : 33.8857\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy at step 4000: 19.6%\n",
      "Minibatch loss at step 5000 : 28.2378\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy at step 5000: 21.8%\n",
      "Test accuracy at step 5000: 21.5%\n",
      "Minibatch loss at step 6000 : 23.4264\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy at step 6000: 25.4%\n",
      "Minibatch loss at step 7000 : 19.6933\n",
      "Minibatch accuracy: 31.2%\n",
      "Validation accuracy at step 7000: 29.0%\n",
      "Minibatch loss at step 8000 : 16.7878\n",
      "Minibatch accuracy: 28.1%\n",
      "Validation accuracy at step 8000: 33.5%\n",
      "Minibatch loss at step 9000 : 13.6957\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 9000: 37.3%\n",
      "Minibatch loss at step 10000 : 11.5759\n",
      "Minibatch accuracy: 28.1%\n",
      "Validation accuracy at step 10000: 38.6%\n",
      "Test accuracy at step 10000: 39.0%\n",
      "Minibatch loss at step 11000 : 9.70278\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 11000: 39.4%\n",
      "Minibatch loss at step 12000 : 8.44864\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 12000: 41.3%\n",
      "Minibatch loss at step 13000 : 7.18024\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 13000: 41.8%\n",
      "Minibatch loss at step 14000 : 6.3923\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 14000: 42.5%\n",
      "Minibatch loss at step 15000 : 5.54193\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy at step 15000: 43.2%\n",
      "Test accuracy at step 15000: 43.7%\n",
      "Minibatch loss at step 16000 : 4.80395\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 16000: 38.7%\n",
      "Minibatch loss at step 17000 : 4.33754\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 17000: 44.5%\n",
      "Minibatch loss at step 18000 : 3.75502\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 18000: 42.2%\n",
      "Minibatch loss at step 19000 : 3.75477\n",
      "Minibatch accuracy: 34.4%\n",
      "Validation accuracy at step 19000: 46.3%\n",
      "Minibatch loss at step 20000 : 3.23572\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy at step 20000: 46.9%\n",
      "Test accuracy at step 20000: 47.3%\n",
      "Minibatch loss at step 21000 : 3.12351\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy at step 21000: 42.7%\n",
      "Minibatch loss at step 22000 : 2.82981\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 22000: 46.9%\n",
      "Minibatch loss at step 23000 : 2.66075\n",
      "Minibatch accuracy: 28.1%\n",
      "Validation accuracy at step 23000: 49.3%\n",
      "Minibatch loss at step 24000 : 2.36408\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 24000: 50.6%\n",
      "Minibatch loss at step 25000 : 2.44225\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 25000: 48.5%\n",
      "Test accuracy at step 25000: 50.1%\n",
      "Minibatch loss at step 26000 : 2.12395\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy at step 26000: 48.6%\n",
      "Minibatch loss at step 27000 : 1.64038\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 27000: 52.0%\n",
      "Minibatch loss at step 28000 : 1.87447\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 28000: 50.9%\n",
      "Minibatch loss at step 29000 : 1.97113\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 29000: 52.7%\n",
      "Minibatch loss at step 30000 : 1.70956\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 30000: 53.6%\n",
      "Test accuracy at step 30000: 54.4%\n",
      "Minibatch loss at step 31000 : 1.78421\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 31000: 53.4%\n",
      "Minibatch loss at step 32000 : 1.80546\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 32000: 49.3%\n",
      "Minibatch loss at step 33000 : 1.53038\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 33000: 54.0%\n",
      "Minibatch loss at step 34000 : 1.57773\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 34000: 55.8%\n",
      "Minibatch loss at step 35000 : 1.45754\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 35000: 56.1%\n",
      "Test accuracy at step 35000: 55.7%\n",
      "Minibatch loss at step 36000 : 1.95247\n",
      "Minibatch accuracy: 31.2%\n",
      "Validation accuracy at step 36000: 55.5%\n",
      "Minibatch loss at step 37000 : 1.43827\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 37000: 55.4%\n",
      "Minibatch loss at step 38000 : 1.70468\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 38000: 57.6%\n",
      "Minibatch loss at step 39000 : 1.72119\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 39000: 57.8%\n",
      "Minibatch loss at step 40000 : 1.45187\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 40000: 57.6%\n",
      "Test accuracy at step 40000: 57.3%\n",
      "Minibatch loss at step 41000 : 2.07905\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy at step 41000: 56.8%\n",
      "Minibatch loss at step 42000 : 1.3547\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 42000: 58.2%\n",
      "Minibatch loss at step 43000 : 1.52533\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 43000: 59.4%\n",
      "Minibatch loss at step 44000 : 1.36811\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 44000: 59.6%\n",
      "Minibatch loss at step 45000 : 1.47336\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 45000: 56.5%\n",
      "Test accuracy at step 45000: 56.1%\n",
      "Minibatch loss at step 46000 : 1.47899\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 46000: 60.4%\n",
      "Minibatch loss at step 47000 : 1.33635\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 47000: 56.4%\n",
      "Minibatch loss at step 48000 : 1.31921\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 48000: 60.6%\n",
      "Minibatch loss at step 49000 : 1.06889\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 49000: 59.1%\n",
      "Minibatch loss at step 50000 : 1.45822\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 50000: 60.1%\n",
      "Test accuracy at step 50000: 59.8%\n",
      "Minibatch loss at step 51000 : 1.56454\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 51000: 61.2%\n",
      "Minibatch loss at step 52000 : 1.52069\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 52000: 61.0%\n",
      "Minibatch loss at step 53000 : 1.32621\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 53000: 61.5%\n",
      "Minibatch loss at step 54000 : 1.65559\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 54000: 62.0%\n",
      "Minibatch loss at step 55000 : 1.26718\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 55000: 62.3%\n",
      "Test accuracy at step 55000: 61.5%\n",
      "Minibatch loss at step 56000 : 1.51055\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 56000: 62.4%\n",
      "Minibatch loss at step 57000 : 1.33653\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 57000: 63.4%\n",
      "Minibatch loss at step 58000 : 1.53447\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 58000: 62.8%\n",
      "Minibatch loss at step 59000 : 1.08534\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 59000: 62.3%\n",
      "Minibatch loss at step 60000 : 1.34196\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 60000: 62.4%\n",
      "Test accuracy at step 60000: 62.3%\n",
      "Minibatch loss at step 61000 : 1.72666\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 61000: 62.4%\n",
      "Minibatch loss at step 62000 : 1.19114\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 62000: 63.7%\n",
      "Minibatch loss at step 63000 : 1.27072\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 63000: 62.4%\n",
      "Minibatch loss at step 64000 : 1.37038\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 64000: 63.8%\n",
      "Minibatch loss at step 65000 : 1.3127\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 65000: 62.0%\n",
      "Test accuracy at step 65000: 62.6%\n",
      "Minibatch loss at step 66000 : 1.76488\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 66000: 61.9%\n",
      "Minibatch loss at step 67000 : 1.47112\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 67000: 59.4%\n",
      "Minibatch loss at step 68000 : 1.3825\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 68000: 64.3%\n",
      "Minibatch loss at step 69000 : 1.39723\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 69000: 64.0%\n",
      "Minibatch loss at step 70000 : 1.47136\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 70000: 64.1%\n",
      "Test accuracy at step 70000: 64.3%\n",
      "Minibatch loss at step 71000 : 1.31988\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 71000: 63.0%\n",
      "Minibatch loss at step 72000 : 1.31218\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 72000: 66.1%\n",
      "Minibatch loss at step 73000 : 1.37996\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 73000: 64.4%\n",
      "Minibatch loss at step 74000 : 1.42113\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 74000: 64.9%\n",
      "Minibatch loss at step 75000 : 1.81275\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 75000: 64.3%\n",
      "Test accuracy at step 75000: 64.5%\n",
      "Minibatch loss at step 76000 : 1.37602\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 76000: 66.0%\n",
      "Minibatch loss at step 77000 : 1.22071\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 77000: 65.2%\n",
      "Minibatch loss at step 78000 : 1.62094\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 78000: 63.3%\n",
      "Minibatch loss at step 79000 : 1.57144\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 79000: 64.1%\n",
      "Minibatch loss at step 80000 : 1.29924\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 80000: 66.1%\n",
      "Test accuracy at step 80000: 66.0%\n",
      "Minibatch loss at step 81000 : 1.26691\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 81000: 65.6%\n",
      "Minibatch loss at step 82000 : 0.974889\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy at step 82000: 66.5%\n",
      "Minibatch loss at step 83000 : 0.941118\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy at step 83000: 66.4%\n",
      "Minibatch loss at step 84000 : 1.37214\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 84000: 63.8%\n",
      "Minibatch loss at step 85000 : 1.13428\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 85000: 65.6%\n",
      "Test accuracy at step 85000: 65.6%\n",
      "\n",
      "End time 19:30\n",
      "The best validation accuracy was 66.54 at step 82000\n",
      "The best test accuracy was 66.01 at step 80000\n"
     ]
    }
   ],
   "source": [
    "model = create_cv_cv_mp_cv_cv_mp_one_hidden_model(learning_rate = 0.001, l2_lambda = 0.1, feature_maps = 16, number_of_hidden_neurons = 64, decay_steps = 10000, decay_rate = 0.96)\n",
    "steps_to_validation_predictions, steps_to_test_predictions = train_model(model, 85001, dropout_keep_prob = 0.9)\n",
    "correct_prediction_indexes, incorrect_prediction_indexes = visualise_accuracies(steps_to_validation_predictions, steps_to_test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The create_two_conv_per_max_pool_one_hidden_model model **slightly improved performance with a validation accuracy of 66.54% and a test accuracy of 66.01%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 364.001\n",
      "Minibatch accuracy: 3.1%\n",
      "Validation accuracy at step 0: 7.7%\n",
      "Minibatch loss at step 1000 : 29.9689\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy at step 1000: 20.7%\n",
      "Minibatch loss at step 2000 : 24.9908\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy at step 2000: 22.9%\n",
      "Minibatch loss at step 3000 : 20.7782\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy at step 3000: 27.9%\n",
      "Minibatch loss at step 4000 : 17.6624\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy at step 4000: 28.3%\n",
      "Minibatch loss at step 5000 : 14.5858\n",
      "Minibatch accuracy: 34.4%\n",
      "Validation accuracy at step 5000: 31.6%\n",
      "Test accuracy at step 5000: 33.0%\n",
      "Minibatch loss at step 6000 : 12.3872\n",
      "Minibatch accuracy: 28.1%\n",
      "Validation accuracy at step 6000: 35.4%\n",
      "Minibatch loss at step 7000 : 10.7749\n",
      "Minibatch accuracy: 34.4%\n",
      "Validation accuracy at step 7000: 37.1%\n",
      "Minibatch loss at step 8000 : 9.12056\n",
      "Minibatch accuracy: 34.4%\n",
      "Validation accuracy at step 8000: 36.6%\n",
      "Minibatch loss at step 9000 : 7.62125\n",
      "Minibatch accuracy: 34.4%\n",
      "Validation accuracy at step 9000: 40.3%\n",
      "Minibatch loss at step 10000 : 6.85376\n",
      "Minibatch accuracy: 34.4%\n",
      "Validation accuracy at step 10000: 40.3%\n",
      "Test accuracy at step 10000: 40.1%\n",
      "Minibatch loss at step 11000 : 5.86027\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 11000: 43.8%\n",
      "Minibatch loss at step 12000 : 5.12077\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy at step 12000: 47.4%\n",
      "Minibatch loss at step 13000 : 4.54369\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 13000: 48.6%\n",
      "Minibatch loss at step 14000 : 3.91576\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 14000: 48.7%\n",
      "Minibatch loss at step 15000 : 3.76449\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 15000: 48.6%\n",
      "Test accuracy at step 15000: 49.1%\n",
      "Minibatch loss at step 16000 : 3.56253\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy at step 16000: 48.9%\n",
      "Minibatch loss at step 17000 : 2.80763\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 17000: 51.4%\n",
      "Minibatch loss at step 18000 : 2.59462\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 18000: 52.8%\n",
      "Minibatch loss at step 19000 : 2.60115\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 19000: 54.1%\n",
      "Minibatch loss at step 20000 : 2.45729\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 20000: 53.8%\n",
      "Test accuracy at step 20000: 53.8%\n",
      "Minibatch loss at step 21000 : 2.17619\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 21000: 54.0%\n",
      "Minibatch loss at step 22000 : 2.4641\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy at step 22000: 54.8%\n",
      "Minibatch loss at step 23000 : 2.25561\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 23000: 57.1%\n",
      "Minibatch loss at step 24000 : 1.80466\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 24000: 57.4%\n",
      "Minibatch loss at step 25000 : 1.78961\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 25000: 57.0%\n",
      "Test accuracy at step 25000: 56.6%\n",
      "Minibatch loss at step 26000 : 1.72693\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 26000: 56.8%\n",
      "Minibatch loss at step 27000 : 1.42275\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 27000: 57.1%\n",
      "Minibatch loss at step 28000 : 1.81318\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 28000: 53.0%\n",
      "Minibatch loss at step 29000 : 1.74098\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 29000: 58.6%\n",
      "Minibatch loss at step 30000 : 1.54268\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 30000: 58.8%\n",
      "Test accuracy at step 30000: 58.5%\n",
      "Minibatch loss at step 31000 : 1.44438\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 31000: 59.2%\n",
      "Minibatch loss at step 32000 : 1.83172\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 32000: 56.8%\n",
      "Minibatch loss at step 33000 : 1.29318\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 33000: 58.0%\n",
      "Minibatch loss at step 34000 : 1.28256\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 34000: 60.3%\n",
      "Minibatch loss at step 35000 : 1.43992\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 35000: 61.4%\n",
      "Test accuracy at step 35000: 60.1%\n",
      "Minibatch loss at step 36000 : 1.662\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 36000: 59.7%\n",
      "Minibatch loss at step 37000 : 1.38062\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 37000: 60.1%\n",
      "Minibatch loss at step 38000 : 1.72474\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 38000: 62.1%\n",
      "Minibatch loss at step 39000 : 1.78743\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 39000: 62.1%\n",
      "Minibatch loss at step 40000 : 1.38486\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 40000: 62.5%\n",
      "Test accuracy at step 40000: 61.6%\n",
      "Minibatch loss at step 41000 : 1.91078\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy at step 41000: 61.8%\n",
      "Minibatch loss at step 42000 : 1.27814\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 42000: 61.9%\n",
      "Minibatch loss at step 43000 : 1.22103\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 43000: 62.6%\n",
      "Minibatch loss at step 44000 : 1.59981\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 44000: 60.0%\n",
      "Minibatch loss at step 45000 : 1.30555\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 45000: 62.8%\n",
      "Test accuracy at step 45000: 62.2%\n",
      "Minibatch loss at step 46000 : 1.53076\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 46000: 62.7%\n",
      "Minibatch loss at step 47000 : 1.40153\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 47000: 61.9%\n",
      "Minibatch loss at step 48000 : 1.23565\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 48000: 61.7%\n",
      "Minibatch loss at step 49000 : 1.02725\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy at step 49000: 63.2%\n",
      "Minibatch loss at step 50000 : 1.50933\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 50000: 61.8%\n",
      "Test accuracy at step 50000: 61.6%\n",
      "Minibatch loss at step 51000 : 1.663\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 51000: 61.9%\n",
      "Minibatch loss at step 52000 : 1.39593\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 52000: 63.0%\n",
      "Minibatch loss at step 53000 : 1.31764\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 53000: 63.6%\n",
      "Minibatch loss at step 54000 : 1.19309\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 54000: 64.0%\n",
      "Minibatch loss at step 55000 : 1.47315\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 55000: 64.5%\n",
      "Test accuracy at step 55000: 64.1%\n",
      "Minibatch loss at step 56000 : 1.58057\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 56000: 64.1%\n",
      "Minibatch loss at step 57000 : 1.389\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 57000: 64.5%\n",
      "Minibatch loss at step 58000 : 1.55596\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 58000: 63.9%\n",
      "Minibatch loss at step 59000 : 0.986271\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy at step 59000: 63.0%\n",
      "Minibatch loss at step 60000 : 1.43009\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 60000: 62.9%\n",
      "Test accuracy at step 60000: 62.3%\n",
      "Minibatch loss at step 61000 : 1.71593\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 61000: 64.4%\n",
      "Minibatch loss at step 62000 : 1.30688\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 62000: 65.2%\n",
      "Minibatch loss at step 63000 : 1.01109\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 63000: 65.0%\n",
      "Minibatch loss at step 64000 : 1.24102\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 64000: 64.7%\n",
      "Minibatch loss at step 65000 : 1.3024\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 65000: 64.0%\n",
      "Test accuracy at step 65000: 63.8%\n",
      "Minibatch loss at step 66000 : 1.51745\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 66000: 62.4%\n",
      "Minibatch loss at step 67000 : 1.36331\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 67000: 61.5%\n",
      "Minibatch loss at step 68000 : 1.25078\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 68000: 65.2%\n",
      "Minibatch loss at step 69000 : 1.55234\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 69000: 65.0%\n",
      "Minibatch loss at step 70000 : 1.31294\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 70000: 65.8%\n",
      "Test accuracy at step 70000: 65.3%\n",
      "Minibatch loss at step 71000 : 1.13871\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 71000: 66.1%\n",
      "Minibatch loss at step 72000 : 1.26806\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 72000: 65.9%\n",
      "Minibatch loss at step 73000 : 1.27764\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 73000: 66.2%\n",
      "Minibatch loss at step 74000 : 1.18149\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 74000: 65.2%\n",
      "Minibatch loss at step 75000 : 1.65801\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 75000: 65.6%\n",
      "Test accuracy at step 75000: 65.0%\n",
      "Minibatch loss at step 76000 : 1.40707\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 76000: 66.2%\n",
      "Minibatch loss at step 77000 : 1.46848\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 77000: 66.1%\n",
      "Minibatch loss at step 78000 : 1.63988\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 78000: 63.4%\n",
      "Minibatch loss at step 79000 : 1.30403\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 79000: 66.1%\n",
      "Minibatch loss at step 80000 : 1.20316\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 80000: 67.0%\n",
      "Test accuracy at step 80000: 66.4%\n",
      "Minibatch loss at step 81000 : 1.31958\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 81000: 65.1%\n",
      "Minibatch loss at step 82000 : 1.04576\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy at step 82000: 67.4%\n",
      "Minibatch loss at step 83000 : 1.31024\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 83000: 66.7%\n",
      "Minibatch loss at step 84000 : 1.52114\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 84000: 65.8%\n",
      "Minibatch loss at step 85000 : 1.05729\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy at step 85000: 67.2%\n",
      "Test accuracy at step 85000: 66.7%\n",
      "\n",
      "Elapsed time: 4.64249284638 hours\n",
      "The best validation accuracy was 67.36 at step 82000\n",
      "The best test accuracy was 66.7 at step 85000\n"
     ]
    }
   ],
   "source": [
    "# Combine the [cn, cn, mp, cn, cn, mp, h] model with the halved decay step of 5000 steps but for only 85K steps in total.\n",
    "model = create_cv_cv_mp_cv_cv_mp_one_hidden_model(learning_rate = 0.001, l2_lambda = 0.1, feature_maps = 16, number_of_hidden_neurons = 64, decay_steps = 5000, decay_rate = 0.96)\n",
    "steps_to_validation_predictions, steps_to_test_predictions = train_model(model, 85001, dropout_keep_prob = 0.9)\n",
    "correct_prediction_indexes, incorrect_prediction_indexes = visualise_accuracies(steps_to_validation_predictions, steps_to_test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the halved decay step and the triple double conv layer model **improved validation accuracy to 67.36% and test accuracy to 66.7%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 185.695\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy at step 0: 9.8%\n",
      "Minibatch loss at step 1000 : 35.6711\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy at step 1000: 15.9%\n",
      "Minibatch loss at step 2000 : 29.8574\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy at step 2000: 19.1%\n",
      "Minibatch loss at step 3000 : 24.9603\n",
      "Minibatch accuracy: 21.9%\n",
      "Validation accuracy at step 3000: 24.7%\n",
      "Minibatch loss at step 4000 : 20.9756\n",
      "Minibatch accuracy: 31.2%\n",
      "Validation accuracy at step 4000: 26.6%\n",
      "Minibatch loss at step 5000 : 17.8353\n",
      "Minibatch accuracy: 21.9%\n",
      "Validation accuracy at step 5000: 30.6%\n",
      "Test accuracy at step 5000: 31.0%\n",
      "Minibatch loss at step 6000 : 15.2424\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy at step 6000: 31.7%\n",
      "Minibatch loss at step 7000 : 13.0257\n",
      "Minibatch accuracy: 34.4%\n",
      "Validation accuracy at step 7000: 34.2%\n",
      "Minibatch loss at step 8000 : 11.3852\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy at step 8000: 36.1%\n",
      "Minibatch loss at step 9000 : 9.83294\n",
      "Minibatch accuracy: 31.2%\n",
      "Validation accuracy at step 9000: 37.8%\n",
      "Minibatch loss at step 10000 : 8.42112\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 10000: 39.2%\n",
      "Test accuracy at step 10000: 40.4%\n",
      "Minibatch loss at step 11000 : 7.48349\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy at step 11000: 38.4%\n",
      "Minibatch loss at step 12000 : 6.48217\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 12000: 41.5%\n",
      "Minibatch loss at step 13000 : 5.7783\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 13000: 42.2%\n",
      "Minibatch loss at step 14000 : 5.35872\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy at step 14000: 42.4%\n",
      "Minibatch loss at step 15000 : 4.99327\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy at step 15000: 43.6%\n",
      "Test accuracy at step 15000: 43.7%\n",
      "Minibatch loss at step 16000 : 4.44872\n",
      "Minibatch accuracy: 28.1%\n",
      "Validation accuracy at step 16000: 44.6%\n",
      "Minibatch loss at step 17000 : 3.85344\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 17000: 46.4%\n",
      "Minibatch loss at step 18000 : 3.59484\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 18000: 47.3%\n",
      "Minibatch loss at step 19000 : 3.58076\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 19000: 47.7%\n",
      "Minibatch loss at step 20000 : 3.19509\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 20000: 47.4%\n",
      "Test accuracy at step 20000: 47.9%\n",
      "Minibatch loss at step 21000 : 3.13475\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 21000: 48.2%\n",
      "Minibatch loss at step 22000 : 2.92567\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy at step 22000: 47.1%\n",
      "Minibatch loss at step 23000 : 2.87935\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy at step 23000: 50.0%\n",
      "Minibatch loss at step 24000 : 2.51252\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 24000: 49.6%\n",
      "Minibatch loss at step 25000 : 2.56919\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy at step 25000: 50.9%\n",
      "Test accuracy at step 25000: 50.7%\n",
      "Minibatch loss at step 26000 : 2.50539\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 26000: 49.1%\n",
      "Minibatch loss at step 27000 : 1.93273\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 27000: 52.0%\n",
      "Minibatch loss at step 28000 : 2.25\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 28000: 51.2%\n",
      "Minibatch loss at step 29000 : 2.14002\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 29000: 51.6%\n",
      "Minibatch loss at step 30000 : 2.15654\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 30000: 52.7%\n",
      "Test accuracy at step 30000: 52.6%\n",
      "Minibatch loss at step 31000 : 1.95195\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 31000: 52.3%\n",
      "Minibatch loss at step 32000 : 2.15844\n",
      "Minibatch accuracy: 34.4%\n",
      "Validation accuracy at step 32000: 51.3%\n",
      "Minibatch loss at step 33000 : 1.77235\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 33000: 51.2%\n",
      "Minibatch loss at step 34000 : 1.77595\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 34000: 53.5%\n",
      "Minibatch loss at step 35000 : 1.54955\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 35000: 53.5%\n",
      "Test accuracy at step 35000: 53.4%\n",
      "Minibatch loss at step 36000 : 2.11537\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 36000: 54.0%\n",
      "Minibatch loss at step 37000 : 1.53708\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 37000: 53.0%\n",
      "Minibatch loss at step 38000 : 1.60978\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 38000: 54.7%\n",
      "Minibatch loss at step 39000 : 2.16088\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 39000: 54.8%\n",
      "Minibatch loss at step 40000 : 1.89715\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy at step 40000: 54.5%\n",
      "Test accuracy at step 40000: 54.6%\n",
      "Minibatch loss at step 41000 : 2.11898\n",
      "Minibatch accuracy: 28.1%\n",
      "Validation accuracy at step 41000: 54.7%\n",
      "Minibatch loss at step 42000 : 1.57601\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 42000: 55.1%\n",
      "Minibatch loss at step 43000 : 1.46125\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 43000: 56.2%\n",
      "Minibatch loss at step 44000 : 1.70086\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 44000: 55.0%\n",
      "Minibatch loss at step 45000 : 1.5199\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 45000: 55.2%\n",
      "Test accuracy at step 45000: 54.4%\n",
      "Minibatch loss at step 46000 : 1.67325\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 46000: 56.8%\n",
      "Minibatch loss at step 47000 : 1.82455\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 47000: 56.2%\n",
      "Minibatch loss at step 48000 : 1.4068\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 48000: 55.5%\n",
      "Minibatch loss at step 49000 : 1.21762\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 49000: 54.8%\n",
      "Minibatch loss at step 50000 : 1.90262\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy at step 50000: 55.9%\n",
      "Test accuracy at step 50000: 56.3%\n",
      "Minibatch loss at step 51000 : 1.69706\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 51000: 56.2%\n",
      "Minibatch loss at step 52000 : 1.52244\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 52000: 56.8%\n",
      "Minibatch loss at step 53000 : 1.32902\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 53000: 57.6%\n",
      "Minibatch loss at step 54000 : 1.54954\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 54000: 57.0%\n",
      "Minibatch loss at step 55000 : 1.48778\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 55000: 57.7%\n",
      "Test accuracy at step 55000: 57.9%\n",
      "Minibatch loss at step 56000 : 1.64263\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 56000: 58.4%\n",
      "Minibatch loss at step 57000 : 1.58094\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 57000: 58.2%\n",
      "Minibatch loss at step 58000 : 1.45612\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 58000: 57.9%\n",
      "Minibatch loss at step 59000 : 1.26559\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 59000: 58.2%\n",
      "Minibatch loss at step 60000 : 1.38777\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 60000: 57.5%\n",
      "Test accuracy at step 60000: 57.5%\n",
      "Minibatch loss at step 61000 : 1.6913\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 61000: 58.2%\n",
      "Minibatch loss at step 62000 : 1.39963\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 62000: 58.3%\n",
      "Minibatch loss at step 63000 : 1.3426\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 63000: 58.4%\n",
      "Minibatch loss at step 64000 : 1.37222\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 64000: 58.4%\n",
      "Minibatch loss at step 65000 : 1.21678\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 65000: 58.7%\n",
      "Test accuracy at step 65000: 58.7%\n",
      "Minibatch loss at step 66000 : 1.54583\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 66000: 58.4%\n",
      "Minibatch loss at step 67000 : 1.62274\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 67000: 56.2%\n",
      "Minibatch loss at step 68000 : 1.25047\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 68000: 58.5%\n",
      "Minibatch loss at step 69000 : 1.38239\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 69000: 59.7%\n",
      "Minibatch loss at step 70000 : 1.41883\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 70000: 58.4%\n",
      "Test accuracy at step 70000: 58.3%\n",
      "Minibatch loss at step 71000 : 1.294\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 71000: 59.3%\n",
      "Minibatch loss at step 72000 : 1.23863\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 72000: 59.9%\n",
      "Minibatch loss at step 73000 : 1.4185\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 73000: 59.4%\n",
      "Minibatch loss at step 74000 : 1.18924\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 74000: 59.9%\n",
      "Minibatch loss at step 75000 : 1.84049\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy at step 75000: 59.8%\n",
      "Test accuracy at step 75000: 59.2%\n",
      "Minibatch loss at step 76000 : 1.33526\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 76000: 59.4%\n",
      "Minibatch loss at step 77000 : 1.31686\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 77000: 59.9%\n",
      "Minibatch loss at step 78000 : 1.34282\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 78000: 58.6%\n",
      "Minibatch loss at step 79000 : 1.43268\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 79000: 59.1%\n",
      "Minibatch loss at step 80000 : 1.38148\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 80000: 59.6%\n",
      "Test accuracy at step 80000: 59.4%\n",
      "Minibatch loss at step 81000 : 1.58502\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 81000: 58.1%\n",
      "Minibatch loss at step 82000 : 1.27345\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 82000: 60.1%\n",
      "Minibatch loss at step 83000 : 1.2764\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 83000: 59.8%\n",
      "Minibatch loss at step 84000 : 1.48719\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 84000: 59.3%\n",
      "Minibatch loss at step 85000 : 1.32907\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 85000: 60.5%\n",
      "Test accuracy at step 85000: 59.2%\n",
      "\n",
      "Elapsed time: 4.83320199834 hours\n",
      "The best validation accuracy was 60.46 at step 85000\n",
      "The best test accuracy was 59.42 at step 80000\n"
     ]
    }
   ],
   "source": [
    "# Combine the [cn, cn, mp, cn, cn, mp, h] model with a decay rate of 0.9 and the halved decay step of 5000 steps.\n",
    "model = create_cv_cv_mp_cv_cv_mp_one_hidden_model(learning_rate = 0.001, l2_lambda = 0.1, feature_maps = 16, number_of_hidden_neurons = 64, decay_steps = 5000, decay_rate = 0.9)\n",
    "steps_to_validation_predictions, steps_to_test_predictions = train_model(model, 85001, dropout_keep_prob = 0.9)\n",
    "correct_prediction_indexes, incorrect_prediction_indexes = visualise_accuracies(steps_to_validation_predictions, steps_to_test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decreasing the decay_rate to 0.9 decreased performance at 85K steps. This could be because the model takes longer to train with a lowered learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 69.2448\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy at step 0: 9.6%\n",
      "Minibatch loss at step 1000 : 17.6272\n",
      "Minibatch accuracy: 28.1%\n",
      "Validation accuracy at step 1000: 24.7%\n",
      "Minibatch loss at step 2000 : 14.8427\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy at step 2000: 29.3%\n",
      "Minibatch loss at step 3000 : 12.4149\n",
      "Minibatch accuracy: 28.1%\n",
      "Validation accuracy at step 3000: 32.5%\n",
      "Minibatch loss at step 4000 : 10.7745\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy at step 4000: 31.2%\n",
      "Minibatch loss at step 5000 : 9.10808\n",
      "Minibatch accuracy: 34.4%\n",
      "Validation accuracy at step 5000: 37.9%\n",
      "Test accuracy at step 5000: 37.5%\n",
      "Minibatch loss at step 6000 : 7.62837\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy at step 6000: 38.9%\n",
      "Minibatch loss at step 7000 : 6.59467\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 7000: 40.5%\n",
      "Minibatch loss at step 8000 : 5.96272\n",
      "Minibatch accuracy: 31.2%\n",
      "Validation accuracy at step 8000: 39.3%\n",
      "Minibatch loss at step 9000 : 5.01837\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy at step 9000: 42.0%\n",
      "Minibatch loss at step 10000 : 4.75524\n",
      "Minibatch accuracy: 34.4%\n",
      "Validation accuracy at step 10000: 39.6%\n",
      "Test accuracy at step 10000: 39.0%\n",
      "Minibatch loss at step 11000 : 4.14451\n",
      "Minibatch accuracy: 34.4%\n",
      "Validation accuracy at step 11000: 40.6%\n",
      "Minibatch loss at step 12000 : 3.64559\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 12000: 46.0%\n",
      "Minibatch loss at step 13000 : 3.44501\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 13000: 46.8%\n",
      "Minibatch loss at step 14000 : 3.02917\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 14000: 45.0%\n",
      "Minibatch loss at step 15000 : 2.98325\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 15000: 47.8%\n",
      "Test accuracy at step 15000: 46.8%\n",
      "Minibatch loss at step 16000 : 2.93974\n",
      "Minibatch accuracy: 31.2%\n",
      "Validation accuracy at step 16000: 47.1%\n",
      "Minibatch loss at step 17000 : 2.32862\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 17000: 49.3%\n",
      "Minibatch loss at step 18000 : 2.22596\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 18000: 49.6%\n",
      "Minibatch loss at step 19000 : 2.5682\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy at step 19000: 49.7%\n",
      "Minibatch loss at step 20000 : 2.22544\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 20000: 49.2%\n",
      "Test accuracy at step 20000: 49.1%\n",
      "Minibatch loss at step 21000 : 1.93803\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 21000: 47.0%\n",
      "Minibatch loss at step 22000 : 2.21603\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 22000: 49.4%\n",
      "Minibatch loss at step 23000 : 2.16661\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy at step 23000: 51.9%\n",
      "Minibatch loss at step 24000 : 1.99246\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 24000: 51.8%\n",
      "Minibatch loss at step 25000 : 1.87878\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 25000: 52.0%\n",
      "Test accuracy at step 25000: 52.1%\n",
      "Minibatch loss at step 26000 : 1.82723\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 26000: 51.4%\n",
      "Minibatch loss at step 27000 : 1.56597\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 27000: 51.5%\n",
      "Minibatch loss at step 28000 : 1.84858\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 28000: 51.0%\n",
      "Minibatch loss at step 29000 : 1.71494\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 29000: 53.0%\n",
      "Minibatch loss at step 30000 : 1.68506\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 30000: 54.6%\n",
      "Test accuracy at step 30000: 54.4%\n",
      "Minibatch loss at step 31000 : 1.88939\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy at step 31000: 54.4%\n",
      "Minibatch loss at step 32000 : 1.9296\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy at step 32000: 50.1%\n",
      "Minibatch loss at step 33000 : 1.57301\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 33000: 50.6%\n",
      "Minibatch loss at step 34000 : 1.52922\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 34000: 53.2%\n",
      "Minibatch loss at step 35000 : 1.45612\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 35000: 56.0%\n",
      "Test accuracy at step 35000: 55.9%\n",
      "Minibatch loss at step 36000 : 1.85312\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 36000: 54.4%\n",
      "Minibatch loss at step 37000 : 1.46887\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 37000: 53.0%\n",
      "Minibatch loss at step 38000 : 1.85929\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 38000: 55.0%\n",
      "Minibatch loss at step 39000 : 2.06318\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 39000: 56.8%\n",
      "Minibatch loss at step 40000 : 1.59518\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 40000: 56.5%\n",
      "Test accuracy at step 40000: 56.6%\n",
      "Minibatch loss at step 41000 : 1.94715\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 41000: 55.7%\n",
      "Minibatch loss at step 42000 : 1.35665\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 42000: 55.6%\n",
      "Minibatch loss at step 43000 : 1.55466\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 43000: 55.9%\n",
      "Minibatch loss at step 44000 : 1.64768\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 44000: 57.5%\n",
      "Minibatch loss at step 45000 : 1.57011\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 45000: 57.1%\n",
      "Test accuracy at step 45000: 57.5%\n",
      "Minibatch loss at step 46000 : 1.63092\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 46000: 57.3%\n",
      "Minibatch loss at step 47000 : 1.78802\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 47000: 55.0%\n",
      "Minibatch loss at step 48000 : 1.47881\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 48000: 56.8%\n",
      "Minibatch loss at step 49000 : 1.24921\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 49000: 57.2%\n",
      "Minibatch loss at step 50000 : 1.80969\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 50000: 56.8%\n",
      "Test accuracy at step 50000: 57.5%\n",
      "Minibatch loss at step 51000 : 1.7331\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 51000: 58.0%\n",
      "Minibatch loss at step 52000 : 1.5817\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 52000: 54.7%\n",
      "Minibatch loss at step 53000 : 1.32724\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 53000: 55.7%\n",
      "Minibatch loss at step 54000 : 1.47662\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 54000: 57.5%\n",
      "Minibatch loss at step 55000 : 1.61634\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 55000: 56.4%\n",
      "Test accuracy at step 55000: 56.4%\n",
      "Minibatch loss at step 56000 : 1.60932\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy at step 56000: 58.5%\n",
      "Minibatch loss at step 57000 : 1.64278\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 57000: 59.7%\n",
      "Minibatch loss at step 58000 : 1.64043\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 58000: 58.4%\n",
      "Minibatch loss at step 59000 : 1.40027\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 59000: 57.5%\n",
      "Minibatch loss at step 60000 : 1.46936\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 60000: 58.9%\n",
      "Test accuracy at step 60000: 58.9%\n",
      "Minibatch loss at step 61000 : 1.98757\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 61000: 59.9%\n",
      "Minibatch loss at step 62000 : 1.44383\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 62000: 59.1%\n",
      "Minibatch loss at step 63000 : 1.54566\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 63000: 59.4%\n",
      "Minibatch loss at step 64000 : 1.55629\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 64000: 58.8%\n",
      "Minibatch loss at step 65000 : 1.50606\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 65000: 57.2%\n",
      "Test accuracy at step 65000: 58.1%\n",
      "Minibatch loss at step 66000 : 1.76911\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 66000: 58.6%\n",
      "Minibatch loss at step 67000 : 1.62279\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 67000: 52.2%\n",
      "Minibatch loss at step 68000 : 1.38943\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 68000: 59.0%\n",
      "Minibatch loss at step 69000 : 1.65259\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 69000: 59.3%\n",
      "Minibatch loss at step 70000 : 1.53026\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 70000: 60.1%\n",
      "Test accuracy at step 70000: 59.9%\n",
      "Minibatch loss at step 71000 : 1.36936\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 71000: 60.0%\n",
      "Minibatch loss at step 72000 : 1.434\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 72000: 60.2%\n",
      "Minibatch loss at step 73000 : 1.40579\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 73000: 59.4%\n",
      "Minibatch loss at step 74000 : 1.45831\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 74000: 59.9%\n",
      "Minibatch loss at step 75000 : 1.84151\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 75000: 58.7%\n",
      "Test accuracy at step 75000: 58.7%\n",
      "Minibatch loss at step 76000 : 1.43146\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 76000: 59.6%\n",
      "Minibatch loss at step 77000 : 1.52789\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 77000: 60.4%\n",
      "Minibatch loss at step 78000 : 1.82873\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 78000: 59.4%\n",
      "Minibatch loss at step 79000 : 1.48413\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 79000: 61.3%\n",
      "Minibatch loss at step 80000 : 1.41958\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 80000: 61.7%\n",
      "Test accuracy at step 80000: 61.4%\n",
      "Minibatch loss at step 81000 : 1.44728\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 81000: 60.2%\n",
      "Minibatch loss at step 82000 : 1.22255\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 82000: 61.2%\n",
      "Minibatch loss at step 83000 : 1.58477\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 83000: 61.1%\n",
      "Minibatch loss at step 84000 : 1.6124\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 84000: 59.0%\n",
      "Minibatch loss at step 85000 : 1.2649\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 85000: 62.0%\n",
      "Test accuracy at step 85000: 61.5%\n",
      "Minibatch loss at step 86000 : 1.41683\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 86000: 59.2%\n",
      "Minibatch loss at step 87000 : 1.45309\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 87000: 60.6%\n",
      "Minibatch loss at step 88000 : 1.40855\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 88000: 60.8%\n",
      "Minibatch loss at step 89000 : 1.60833\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 89000: 61.9%\n",
      "Minibatch loss at step 90000 : 1.34982\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 90000: 62.1%\n",
      "Test accuracy at step 90000: 61.8%\n",
      "Minibatch loss at step 91000 : 1.65047\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 91000: 60.5%\n",
      "Minibatch loss at step 92000 : 1.37599\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 92000: 62.2%\n",
      "Minibatch loss at step 93000 : 1.60273\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 93000: 62.0%\n",
      "Minibatch loss at step 94000 : 1.45877\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 94000: 61.7%\n",
      "Minibatch loss at step 95000 : 1.40936\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 95000: 62.2%\n",
      "Test accuracy at step 95000: 62.1%\n",
      "Minibatch loss at step 96000 : 1.49421\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 96000: 62.6%\n",
      "Minibatch loss at step 97000 : 1.17514\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 97000: 62.6%\n",
      "Minibatch loss at step 98000 : 1.38692\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 98000: 62.7%\n",
      "Minibatch loss at step 99000 : 1.25913\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 99000: 59.4%\n",
      "Minibatch loss at step 100000 : 1.33215\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 100000: 62.4%\n",
      "Test accuracy at step 100000: 62.4%\n",
      "Minibatch loss at step 101000 : 1.4222\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 101000: 60.6%\n",
      "Minibatch loss at step 102000 : 1.19112\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 102000: 61.8%\n",
      "Minibatch loss at step 103000 : 1.47018\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 103000: 63.4%\n",
      "Minibatch loss at step 104000 : 1.53743\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 104000: 61.8%\n",
      "Minibatch loss at step 105000 : 1.39033\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 105000: 62.4%\n",
      "Test accuracy at step 105000: 62.0%\n",
      "Minibatch loss at step 106000 : 1.43335\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 106000: 61.8%\n",
      "Minibatch loss at step 107000 : 1.47811\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 107000: 62.5%\n",
      "Minibatch loss at step 108000 : 1.34295\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 108000: 61.9%\n",
      "Minibatch loss at step 109000 : 1.43646\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 109000: 63.6%\n",
      "Minibatch loss at step 110000 : 1.2916\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 110000: 63.0%\n",
      "Test accuracy at step 110000: 62.8%\n",
      "Minibatch loss at step 111000 : 1.27085\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 111000: 63.6%\n",
      "Minibatch loss at step 112000 : 1.42421\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 112000: 62.4%\n",
      "Minibatch loss at step 113000 : 1.37034\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 113000: 60.0%\n",
      "Minibatch loss at step 114000 : 1.5349\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 114000: 61.9%\n",
      "Minibatch loss at step 115000 : 1.69838\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 115000: 62.6%\n",
      "Test accuracy at step 115000: 62.6%\n",
      "Minibatch loss at step 116000 : 1.84891\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 116000: 62.7%\n",
      "Minibatch loss at step 117000 : 1.61454\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 117000: 63.4%\n",
      "Minibatch loss at step 118000 : 1.23401\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 118000: 63.3%\n",
      "Minibatch loss at step 119000 : 1.40684\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 119000: 63.1%\n",
      "Minibatch loss at step 120000 : 1.50178\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 120000: 62.5%\n",
      "Test accuracy at step 120000: 62.7%\n",
      "Minibatch loss at step 121000 : 1.54368\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 121000: 62.2%\n",
      "Minibatch loss at step 122000 : 1.39225\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 122000: 62.9%\n",
      "Minibatch loss at step 123000 : 1.52326\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 123000: 63.9%\n",
      "Minibatch loss at step 124000 : 1.46659\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 124000: 63.7%\n",
      "Minibatch loss at step 125000 : 1.47845\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 125000: 63.6%\n",
      "Test accuracy at step 125000: 64.2%\n",
      "Minibatch loss at step 126000 : 1.43835\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 126000: 64.0%\n",
      "Minibatch loss at step 127000 : 1.55655\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 127000: 62.9%\n",
      "Minibatch loss at step 128000 : 1.5364\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 128000: 62.4%\n",
      "Minibatch loss at step 129000 : 1.22045\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 129000: 63.8%\n",
      "Minibatch loss at step 130000 : 1.68273\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy at step 130000: 61.6%\n",
      "Test accuracy at step 130000: 61.5%\n",
      "Minibatch loss at step 131000 : 1.63062\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 131000: 63.4%\n",
      "Minibatch loss at step 132000 : 1.115\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy at step 132000: 63.9%\n",
      "Minibatch loss at step 133000 : 1.14193\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 133000: 63.6%\n",
      "Minibatch loss at step 134000 : 1.41387\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 134000: 63.6%\n",
      "Minibatch loss at step 135000 : 1.62982\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 135000: 63.7%\n",
      "Test accuracy at step 135000: 64.4%\n",
      "Minibatch loss at step 136000 : 1.53182\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 136000: 63.6%\n",
      "Minibatch loss at step 137000 : 1.63123\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 137000: 63.7%\n",
      "Minibatch loss at step 138000 : 2.01702\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 138000: 63.3%\n",
      "Minibatch loss at step 139000 : 1.4593\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 139000: 60.6%\n",
      "Minibatch loss at step 140000 : 1.44063\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 140000: 63.6%\n",
      "Test accuracy at step 140000: 63.2%\n",
      "Minibatch loss at step 141000 : 1.67747\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 141000: 63.8%\n",
      "Minibatch loss at step 142000 : 1.27977\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 142000: 63.5%\n",
      "Minibatch loss at step 143000 : 1.44795\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 143000: 63.5%\n",
      "Minibatch loss at step 144000 : 1.48092\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 144000: 63.6%\n",
      "Minibatch loss at step 145000 : 1.05339\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy at step 145000: 61.8%\n",
      "Test accuracy at step 145000: 61.8%\n",
      "Minibatch loss at step 146000 : 1.67455\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 146000: 62.9%\n",
      "Minibatch loss at step 147000 : 1.41331\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 147000: 64.5%\n",
      "Minibatch loss at step 148000 : 1.29084\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 148000: 64.7%\n",
      "Minibatch loss at step 149000 : 1.49038\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 149000: 63.0%\n",
      "Minibatch loss at step 150000 : 1.38846\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 150000: 64.5%\n",
      "Test accuracy at step 150000: 64.4%\n",
      "Minibatch loss at step 151000 : 1.10656\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy at step 151000: 63.8%\n",
      "Minibatch loss at step 152000 : 1.85917\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 152000: 64.5%\n",
      "Minibatch loss at step 153000 : 1.53294\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 153000: 64.7%\n",
      "Minibatch loss at step 154000 : 1.36461\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 154000: 64.7%\n",
      "Minibatch loss at step 155000 : 1.4465\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 155000: 63.7%\n",
      "Test accuracy at step 155000: 63.9%\n",
      "Minibatch loss at step 156000 : 1.47924\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 156000: 64.4%\n",
      "Minibatch loss at step 157000 : 1.43941\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 157000: 64.5%\n",
      "Minibatch loss at step 158000 : 1.20274\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy at step 158000: 65.0%\n",
      "Minibatch loss at step 159000 : 1.33076\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 159000: 64.4%\n",
      "Minibatch loss at step 160000 : 1.39645\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 160000: 64.7%\n",
      "Test accuracy at step 160000: 64.8%\n",
      "Minibatch loss at step 161000 : 1.23677\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 161000: 64.2%\n",
      "Minibatch loss at step 162000 : 1.42915\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 162000: 62.9%\n",
      "Minibatch loss at step 163000 : 1.51004\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 163000: 62.9%\n",
      "Minibatch loss at step 164000 : 1.43661\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 164000: 62.5%\n",
      "Minibatch loss at step 165000 : 1.46326\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 165000: 60.9%\n",
      "Test accuracy at step 165000: 61.5%\n",
      "Minibatch loss at step 166000 : 1.45942\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 166000: 62.8%\n",
      "Minibatch loss at step 167000 : 1.32576\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 167000: 65.2%\n",
      "Minibatch loss at step 168000 : 1.6261\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 168000: 64.6%\n",
      "Minibatch loss at step 169000 : 1.5335\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 169000: 65.3%\n",
      "Minibatch loss at step 170000 : 1.17527\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 170000: 64.1%\n",
      "Test accuracy at step 170000: 64.8%\n",
      "Minibatch loss at step 171000 : 1.21036\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 171000: 64.9%\n",
      "Minibatch loss at step 172000 : 1.18483\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 172000: 64.8%\n",
      "Minibatch loss at step 173000 : 1.50342\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 173000: 64.6%\n",
      "Minibatch loss at step 174000 : 1.24174\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 174000: 64.8%\n",
      "Minibatch loss at step 175000 : 1.74066\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 175000: 64.3%\n",
      "Test accuracy at step 175000: 64.7%\n",
      "Minibatch loss at step 176000 : 1.31332\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 176000: 65.2%\n",
      "Minibatch loss at step 177000 : 1.45282\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 177000: 64.6%\n",
      "Minibatch loss at step 178000 : 1.63452\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 178000: 65.0%\n",
      "Minibatch loss at step 179000 : 1.74026\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 179000: 65.5%\n",
      "Minibatch loss at step 180000 : 1.26827\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 180000: 65.4%\n",
      "Test accuracy at step 180000: 65.6%\n",
      "Minibatch loss at step 181000 : 1.32998\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 181000: 64.8%\n",
      "Minibatch loss at step 182000 : 1.35904\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 182000: 64.9%\n",
      "Minibatch loss at step 183000 : 1.53564\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 183000: 64.2%\n",
      "Minibatch loss at step 184000 : 1.51333\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 184000: 63.8%\n",
      "Minibatch loss at step 185000 : 1.38838\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 185000: 63.7%\n",
      "Test accuracy at step 185000: 63.5%\n",
      "Minibatch loss at step 186000 : 1.33909\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 186000: 65.4%\n",
      "Minibatch loss at step 187000 : 1.43889\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 187000: 65.3%\n",
      "Minibatch loss at step 188000 : 1.51001\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 188000: 65.0%\n",
      "Minibatch loss at step 189000 : 1.27309\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 189000: 65.3%\n",
      "Minibatch loss at step 190000 : 1.57382\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 190000: 64.7%\n",
      "Test accuracy at step 190000: 64.0%\n",
      "Minibatch loss at step 191000 : 1.47527\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 191000: 65.6%\n",
      "Minibatch loss at step 192000 : 1.13799\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy at step 192000: 64.7%\n",
      "Minibatch loss at step 193000 : 1.44731\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 193000: 63.7%\n",
      "Minibatch loss at step 194000 : 1.59739\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 194000: 65.1%\n",
      "Minibatch loss at step 195000 : 1.52471\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 195000: 63.8%\n",
      "Test accuracy at step 195000: 63.5%\n",
      "Minibatch loss at step 196000 : 1.3895\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 196000: 64.9%\n",
      "Minibatch loss at step 197000 : 1.52089\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 197000: 65.3%\n",
      "Minibatch loss at step 198000 : 1.24152\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 198000: 65.3%\n",
      "Minibatch loss at step 199000 : 1.17392\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 199000: 64.5%\n",
      "Minibatch loss at step 200000 : 1.66435\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 200000: 65.3%\n",
      "Test accuracy at step 200000: 65.0%\n",
      "Minibatch loss at step 201000 : 1.53248\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 201000: 65.4%\n",
      "Minibatch loss at step 202000 : 1.69855\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 202000: 65.3%\n",
      "Minibatch loss at step 203000 : 1.33931\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 203000: 65.6%\n",
      "Minibatch loss at step 204000 : 1.47766\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 204000: 64.9%\n",
      "Minibatch loss at step 205000 : 1.70116\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 205000: 65.8%\n",
      "Test accuracy at step 205000: 65.7%\n",
      "Minibatch loss at step 206000 : 1.28999\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 206000: 65.6%\n",
      "Minibatch loss at step 207000 : 1.42806\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 207000: 65.9%\n",
      "Minibatch loss at step 208000 : 1.38092\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 208000: 65.6%\n",
      "Minibatch loss at step 209000 : 1.26492\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 209000: 65.1%\n",
      "Minibatch loss at step 210000 : 1.7313\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 210000: 65.4%\n",
      "Test accuracy at step 210000: 65.7%\n",
      "Minibatch loss at step 211000 : 1.30734\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 211000: 65.7%\n",
      "Minibatch loss at step 212000 : 1.15745\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 212000: 65.6%\n",
      "Minibatch loss at step 213000 : 1.43451\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 213000: 65.9%\n",
      "Minibatch loss at step 214000 : 1.50539\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 214000: 65.4%\n",
      "Minibatch loss at step 215000 : 1.17014\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 215000: 65.7%\n",
      "Test accuracy at step 215000: 65.9%\n",
      "Minibatch loss at step 216000 : 1.61115\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 216000: 66.1%\n",
      "Minibatch loss at step 217000 : 1.39282\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 217000: 65.8%\n",
      "Minibatch loss at step 218000 : 1.2597\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 218000: 65.7%\n",
      "Minibatch loss at step 219000 : 1.21175\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 219000: 64.6%\n",
      "Minibatch loss at step 220000 : 1.39274\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 220000: 65.8%\n",
      "Test accuracy at step 220000: 65.8%\n",
      "Minibatch loss at step 221000 : 1.62637\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 221000: 65.4%\n",
      "Minibatch loss at step 222000 : 1.19651\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy at step 222000: 65.5%\n",
      "Minibatch loss at step 223000 : 1.36274\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 223000: 65.8%\n",
      "Minibatch loss at step 224000 : 1.75378\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 224000: 65.5%\n",
      "Minibatch loss at step 225000 : 1.26365\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 225000: 65.7%\n",
      "Test accuracy at step 225000: 65.8%\n",
      "Minibatch loss at step 226000 : 1.75567\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy at step 226000: 64.3%\n",
      "Minibatch loss at step 227000 : 1.41057\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 227000: 66.0%\n",
      "Minibatch loss at step 228000 : 1.63825\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy at step 228000: 66.3%\n",
      "Minibatch loss at step 229000 : 1.38552\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 229000: 65.7%\n",
      "Minibatch loss at step 230000 : 1.19198\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy at step 230000: 65.0%\n",
      "Test accuracy at step 230000: 64.7%\n",
      "Minibatch loss at step 231000 : 1.58545\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 231000: 63.9%\n",
      "Minibatch loss at step 232000 : 1.28537\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 232000: 66.0%\n",
      "Minibatch loss at step 233000 : 1.39205\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 233000: 64.7%\n",
      "Minibatch loss at step 234000 : 1.55889\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 234000: 66.1%\n",
      "Minibatch loss at step 235000 : 1.51486\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 235000: 65.4%\n",
      "Test accuracy at step 235000: 65.1%\n",
      "Minibatch loss at step 236000 : 1.63782\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 236000: 65.1%\n",
      "Minibatch loss at step 237000 : 1.46236\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 237000: 65.6%\n",
      "Minibatch loss at step 238000 : 1.42924\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 238000: 65.2%\n",
      "Minibatch loss at step 239000 : 1.60135\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 239000: 64.1%\n",
      "Minibatch loss at step 240000 : 1.08543\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 240000: 66.2%\n",
      "Test accuracy at step 240000: 66.3%\n",
      "Minibatch loss at step 241000 : 1.38043\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 241000: 66.3%\n",
      "Minibatch loss at step 242000 : 1.32337\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 242000: 65.0%\n",
      "Minibatch loss at step 243000 : 1.47175\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 243000: 65.9%\n",
      "Minibatch loss at step 244000 : 1.33432\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy at step 244000: 66.0%\n",
      "Minibatch loss at step 245000 : 1.31655\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy at step 245000: 65.0%\n",
      "Test accuracy at step 245000: 65.0%\n",
      "Minibatch loss at step 246000 : 1.5238\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy at step 246000: 65.3%\n",
      "Minibatch loss at step 247000 : 1.44759\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy at step 247000: 66.3%\n",
      "Minibatch loss at step 248000 : 1.34287\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy at step 248000: 66.2%\n",
      "Minibatch loss at step 249000 : 1.23504\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy at step 249000: 65.7%\n",
      "Minibatch loss at step 250000 : 1.46809\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy at step 250000: 66.3%\n",
      "Test accuracy at step 250000: 66.2%\n",
      "\n",
      "Elapsed time: 15.5231092417 hours\n",
      "The best validation accuracy was 66.35 at step 228000\n",
      "The best test accuracy was 66.27 at step 240000\n"
     ]
    }
   ],
   "source": [
    "# Combine the [cn, cn, mp, cn, cn, mp, cn, cn, mp, h] model 250K steps to see if it will perform better than the previous model which had 2 conv layers and 1 hidden layer.\n",
    "model = create_three_double_conv_layers_one_hidden_model(learning_rate = 0.001, l2_lambda = 0.1, feature_maps = 16, number_of_hidden_neurons = 64, decay_steps = 5000, decay_rate = 0.96)\n",
    "steps_to_validation_predictions, steps_to_test_predictions = train_model(model, 250001, dropout_keep_prob = 0.9)\n",
    "correct_prediction_indexes, incorrect_prediction_indexes = visualise_accuracies(steps_to_validation_predictions, steps_to_test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create_three_double_conv_layers_one_hidden_model results.\n",
    "\n",
    "Increasing the l2_lambda to 0.2 and 0.3 made the model perform worse of test accuracies of 43.74% and 33.31% respectively for 85K steps.\n",
    "\n",
    "A normal l2_lambda of 0.1 for 85K steps produced a test accuracy of 59.55%. Running it for **250K steps** produced a validation accuracy of 66.3% and a test accuracy of 66.27%. Running the model for an extra (250 - 85) steps produced an **improvement of 6.72% for the test dataset**. Compared to the two conv layer model trained for 250K steps, it was **1.03% worse** for the validation dataset and **0.43% worse** for the test dataset.\n",
    "\n",
    "In conclusion the create_three_double_conv_layers_one_hidden_model model performed worse in every respect than the 2 conv layer version. This does not mean that making a network deeper leads to poor performance, but it is likely that deepening the network requires other hyper parameters to be tuned in order to lead to superior performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: learn about weight initialisation so you can initialise weights to not only improve model performance but also to get the triple layer double conv model to work and actually learn.\n",
    "\n",
    "Ideas: allow the pool layers to reduce the dimensions of the data to see if it improves performance.\n",
    "\n",
    "Idea: Create deeper networks.\n",
    "\n",
    "Idea: reduce the number of feature maps for deeper layers.\n",
    "\n",
    "Implement a 5x1 and 1x5 convolutional map and see if it maitains performance whilst decreasing training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 5, 12)\n",
      "(10, 5, 5, 12)\n",
      "TensorShape([Dimension(10), Dimension(5), Dimension(5), Dimension(24)])\n",
      "TensorShape([Dimension(10), Dimension(5), Dimension(5), Dimension(24)])\n",
      "a shape Tensor(\"Const:0\", shape=TensorShape([Dimension(4), Dimension(2), Dimension(2), Dimension(3)]), dtype=int64)\n",
      "Tensor(\"Pad:0\", shape=TensorShape([Dimension(4), Dimension(4), Dimension(6), Dimension(3)]), dtype=int64)\n",
      "5 5\n",
      "input shape [10  5  5 12]\n",
      "target shape [10  5  5 12]\n",
      "Zero pad: padding [[0, 0], [0, 0], [0, 0], [0, 0]]\n",
      "input shape [10  4  4 12]\n",
      "target shape [10  5  5 12]\n",
      "Zero pad: padding [[0, 0], [1, 0], [1, 0], [0, 0]]\n",
      "Tensor(\"concat_1:0\", shape=TensorShape([Dimension(10), Dimension(5), Dimension(5), Dimension(24)]), dtype=int64)\n",
      "[[[[   0    1    2 ...,    0    0    0]\n",
      "   [  12   13   14 ...,    0    0    0]\n",
      "   [  24   25   26 ...,    0    0    0]\n",
      "   [  36   37   38 ...,    0    0    0]\n",
      "   [  48   49   50 ...,    0    0    0]]\n",
      "\n",
      "  [[  60   61   62 ...,    0    0    0]\n",
      "   [  72   73   74 ...,    9   10   11]\n",
      "   [  84   85   86 ...,   21   22   23]\n",
      "   [  96   97   98 ...,   33   34   35]\n",
      "   [ 108  109  110 ...,   45   46   47]]\n",
      "\n",
      "  [[ 120  121  122 ...,    0    0    0]\n",
      "   [ 132  133  134 ...,   57   58   59]\n",
      "   [ 144  145  146 ...,   69   70   71]\n",
      "   [ 156  157  158 ...,   81   82   83]\n",
      "   [ 168  169  170 ...,   93   94   95]]\n",
      "\n",
      "  [[ 180  181  182 ...,    0    0    0]\n",
      "   [ 192  193  194 ...,  105  106  107]\n",
      "   [ 204  205  206 ...,  117  118  119]\n",
      "   [ 216  217  218 ...,  129  130  131]\n",
      "   [ 228  229  230 ...,  141  142  143]]\n",
      "\n",
      "  [[ 240  241  242 ...,    0    0    0]\n",
      "   [ 252  253  254 ...,  153  154  155]\n",
      "   [ 264  265  266 ...,  165  166  167]\n",
      "   [ 276  277  278 ...,  177  178  179]\n",
      "   [ 288  289  290 ...,  189  190  191]]]\n",
      "\n",
      "\n",
      " [[[ 300  301  302 ...,    0    0    0]\n",
      "   [ 312  313  314 ...,    0    0    0]\n",
      "   [ 324  325  326 ...,    0    0    0]\n",
      "   [ 336  337  338 ...,    0    0    0]\n",
      "   [ 348  349  350 ...,    0    0    0]]\n",
      "\n",
      "  [[ 360  361  362 ...,    0    0    0]\n",
      "   [ 372  373  374 ...,  201  202  203]\n",
      "   [ 384  385  386 ...,  213  214  215]\n",
      "   [ 396  397  398 ...,  225  226  227]\n",
      "   [ 408  409  410 ...,  237  238  239]]\n",
      "\n",
      "  [[ 420  421  422 ...,    0    0    0]\n",
      "   [ 432  433  434 ...,  249  250  251]\n",
      "   [ 444  445  446 ...,  261  262  263]\n",
      "   [ 456  457  458 ...,  273  274  275]\n",
      "   [ 468  469  470 ...,  285  286  287]]\n",
      "\n",
      "  [[ 480  481  482 ...,    0    0    0]\n",
      "   [ 492  493  494 ...,  297  298  299]\n",
      "   [ 504  505  506 ...,  309  310  311]\n",
      "   [ 516  517  518 ...,  321  322  323]\n",
      "   [ 528  529  530 ...,  333  334  335]]\n",
      "\n",
      "  [[ 540  541  542 ...,    0    0    0]\n",
      "   [ 552  553  554 ...,  345  346  347]\n",
      "   [ 564  565  566 ...,  357  358  359]\n",
      "   [ 576  577  578 ...,  369  370  371]\n",
      "   [ 588  589  590 ...,  381  382  383]]]\n",
      "\n",
      "\n",
      " [[[ 600  601  602 ...,    0    0    0]\n",
      "   [ 612  613  614 ...,    0    0    0]\n",
      "   [ 624  625  626 ...,    0    0    0]\n",
      "   [ 636  637  638 ...,    0    0    0]\n",
      "   [ 648  649  650 ...,    0    0    0]]\n",
      "\n",
      "  [[ 660  661  662 ...,    0    0    0]\n",
      "   [ 672  673  674 ...,  393  394  395]\n",
      "   [ 684  685  686 ...,  405  406  407]\n",
      "   [ 696  697  698 ...,  417  418  419]\n",
      "   [ 708  709  710 ...,  429  430  431]]\n",
      "\n",
      "  [[ 720  721  722 ...,    0    0    0]\n",
      "   [ 732  733  734 ...,  441  442  443]\n",
      "   [ 744  745  746 ...,  453  454  455]\n",
      "   [ 756  757  758 ...,  465  466  467]\n",
      "   [ 768  769  770 ...,  477  478  479]]\n",
      "\n",
      "  [[ 780  781  782 ...,    0    0    0]\n",
      "   [ 792  793  794 ...,  489  490  491]\n",
      "   [ 804  805  806 ...,  501  502  503]\n",
      "   [ 816  817  818 ...,  513  514  515]\n",
      "   [ 828  829  830 ...,  525  526  527]]\n",
      "\n",
      "  [[ 840  841  842 ...,    0    0    0]\n",
      "   [ 852  853  854 ...,  537  538  539]\n",
      "   [ 864  865  866 ...,  549  550  551]\n",
      "   [ 876  877  878 ...,  561  562  563]\n",
      "   [ 888  889  890 ...,  573  574  575]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[2100 2101 2102 ...,    0    0    0]\n",
      "   [2112 2113 2114 ...,    0    0    0]\n",
      "   [2124 2125 2126 ...,    0    0    0]\n",
      "   [2136 2137 2138 ...,    0    0    0]\n",
      "   [2148 2149 2150 ...,    0    0    0]]\n",
      "\n",
      "  [[2160 2161 2162 ...,    0    0    0]\n",
      "   [2172 2173 2174 ..., 1353 1354 1355]\n",
      "   [2184 2185 2186 ..., 1365 1366 1367]\n",
      "   [2196 2197 2198 ..., 1377 1378 1379]\n",
      "   [2208 2209 2210 ..., 1389 1390 1391]]\n",
      "\n",
      "  [[2220 2221 2222 ...,    0    0    0]\n",
      "   [2232 2233 2234 ..., 1401 1402 1403]\n",
      "   [2244 2245 2246 ..., 1413 1414 1415]\n",
      "   [2256 2257 2258 ..., 1425 1426 1427]\n",
      "   [2268 2269 2270 ..., 1437 1438 1439]]\n",
      "\n",
      "  [[2280 2281 2282 ...,    0    0    0]\n",
      "   [2292 2293 2294 ..., 1449 1450 1451]\n",
      "   [2304 2305 2306 ..., 1461 1462 1463]\n",
      "   [2316 2317 2318 ..., 1473 1474 1475]\n",
      "   [2328 2329 2330 ..., 1485 1486 1487]]\n",
      "\n",
      "  [[2340 2341 2342 ...,    0    0    0]\n",
      "   [2352 2353 2354 ..., 1497 1498 1499]\n",
      "   [2364 2365 2366 ..., 1509 1510 1511]\n",
      "   [2376 2377 2378 ..., 1521 1522 1523]\n",
      "   [2388 2389 2390 ..., 1533 1534 1535]]]\n",
      "\n",
      "\n",
      " [[[2400 2401 2402 ...,    0    0    0]\n",
      "   [2412 2413 2414 ...,    0    0    0]\n",
      "   [2424 2425 2426 ...,    0    0    0]\n",
      "   [2436 2437 2438 ...,    0    0    0]\n",
      "   [2448 2449 2450 ...,    0    0    0]]\n",
      "\n",
      "  [[2460 2461 2462 ...,    0    0    0]\n",
      "   [2472 2473 2474 ..., 1545 1546 1547]\n",
      "   [2484 2485 2486 ..., 1557 1558 1559]\n",
      "   [2496 2497 2498 ..., 1569 1570 1571]\n",
      "   [2508 2509 2510 ..., 1581 1582 1583]]\n",
      "\n",
      "  [[2520 2521 2522 ...,    0    0    0]\n",
      "   [2532 2533 2534 ..., 1593 1594 1595]\n",
      "   [2544 2545 2546 ..., 1605 1606 1607]\n",
      "   [2556 2557 2558 ..., 1617 1618 1619]\n",
      "   [2568 2569 2570 ..., 1629 1630 1631]]\n",
      "\n",
      "  [[2580 2581 2582 ...,    0    0    0]\n",
      "   [2592 2593 2594 ..., 1641 1642 1643]\n",
      "   [2604 2605 2606 ..., 1653 1654 1655]\n",
      "   [2616 2617 2618 ..., 1665 1666 1667]\n",
      "   [2628 2629 2630 ..., 1677 1678 1679]]\n",
      "\n",
      "  [[2640 2641 2642 ...,    0    0    0]\n",
      "   [2652 2653 2654 ..., 1689 1690 1691]\n",
      "   [2664 2665 2666 ..., 1701 1702 1703]\n",
      "   [2676 2677 2678 ..., 1713 1714 1715]\n",
      "   [2688 2689 2690 ..., 1725 1726 1727]]]\n",
      "\n",
      "\n",
      " [[[2700 2701 2702 ...,    0    0    0]\n",
      "   [2712 2713 2714 ...,    0    0    0]\n",
      "   [2724 2725 2726 ...,    0    0    0]\n",
      "   [2736 2737 2738 ...,    0    0    0]\n",
      "   [2748 2749 2750 ...,    0    0    0]]\n",
      "\n",
      "  [[2760 2761 2762 ...,    0    0    0]\n",
      "   [2772 2773 2774 ..., 1737 1738 1739]\n",
      "   [2784 2785 2786 ..., 1749 1750 1751]\n",
      "   [2796 2797 2798 ..., 1761 1762 1763]\n",
      "   [2808 2809 2810 ..., 1773 1774 1775]]\n",
      "\n",
      "  [[2820 2821 2822 ...,    0    0    0]\n",
      "   [2832 2833 2834 ..., 1785 1786 1787]\n",
      "   [2844 2845 2846 ..., 1797 1798 1799]\n",
      "   [2856 2857 2858 ..., 1809 1810 1811]\n",
      "   [2868 2869 2870 ..., 1821 1822 1823]]\n",
      "\n",
      "  [[2880 2881 2882 ...,    0    0    0]\n",
      "   [2892 2893 2894 ..., 1833 1834 1835]\n",
      "   [2904 2905 2906 ..., 1845 1846 1847]\n",
      "   [2916 2917 2918 ..., 1857 1858 1859]\n",
      "   [2928 2929 2930 ..., 1869 1870 1871]]\n",
      "\n",
      "  [[2940 2941 2942 ...,    0    0    0]\n",
      "   [2952 2953 2954 ..., 1881 1882 1883]\n",
      "   [2964 2965 2966 ..., 1893 1894 1895]\n",
      "   [2976 2977 2978 ..., 1905 1906 1907]\n",
      "   [2988 2989 2990 ..., 1917 1918 1919]]]]\n"
     ]
    }
   ],
   "source": [
    "# Implement an inception module. This consists of concurrent layers: 1x1 conv, 3x3 conv, 5x5 conv, 3x3 maxpooling. \n",
    "# Then a concat layer that adds their outputs together.\n",
    "\n",
    "# A description of DepthConcat\n",
    "\"\"\"\n",
    "--[[ DepthConcat ]]--\n",
    "-- Concatenates the output of Convolutions along the depth dimension\n",
    "-- (nOutputFrame). This is used to implement the DepthConcat layer\n",
    "-- of the Going deeper with convolutions paper :\n",
    "-- http://arxiv.org/pdf/1409.4842v1.pdf\n",
    "-- The normal Concat Module can't be used since the spatial dimensions\n",
    "-- of tensors to be concatenated may have different values. To deal with\n",
    "-- this, we select the largest spatial dimensions and add zero-padding\n",
    "-- around the smaller dimensions.\n",
    "\"\"\"\n",
    "\n",
    "# Therefore a Depth Concat is just a concat at the depth dimension.\n",
    "# e.g. DepthConcat([5, 5, 12], [5, 5, 18]) = [5, 5, 30] (this may be wrong)\n",
    "\n",
    "t1 = np.arange(10 * 5 * 5 * 12).reshape((10, 5, 5, 12))\n",
    "print t1.shape\n",
    "t2 = np.arange(10 * 5 * 5 * 12).reshape((10, 5, 5, 12))\n",
    "print t2.shape\n",
    "depth_dim = 3\n",
    "concat_tensor = tf.concat(depth_dim, [t1, t2])\n",
    "print concat_tensor.get_shape()\n",
    "print concat_tensor.get_shape()\n",
    "\n",
    "\n",
    "# Padding code experiment\n",
    "sess = tf.InteractiveSession()\n",
    "a = tf.constant(np.arange(4 * 2 * 2 * 3).reshape((4, 2, 2, 3)))\n",
    "print \"a shape %s\" % a\n",
    "\n",
    "top = 1\n",
    "bottom = 1\n",
    "left = 2\n",
    "right = 2\n",
    "padding = tf.constant([[0, 0], [top, bottom], [left, right], [0, 0]])\n",
    "result = tf.pad(a, padding)\n",
    "print result\n",
    "#print result.eval()\n",
    "\n",
    "from neural_network import depth_concat\n",
    "    \n",
    "t3 = tf.Variable(np.arange(10 * 5 * 5 * 12).reshape((10, 5, 5, 12)))\n",
    "t4 = tf.Variable(np.arange(10 * 4 * 4 * 12).reshape((10, 4, 4, 12)))\n",
    "depth_concat_layer = depth_concat([t3, t4])\n",
    "print depth_concat_layer\n",
    "tf.initialize_all_variables().run()\n",
    "print depth_concat_layer.eval()\n",
    "\n",
    "# Turn this into an external file and write some unit tests.\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_inception_module_model(learning_rate = 0.05, initialised_weights_stddev = 0.1, feature_maps = 16, number_of_hidden_neurons = 64, batch_size = 32, l2_lambda = 0.1, decay_steps = 10000, decay_rate = 0.96):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "        # Input data.\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "        dropout_keep_probability = tf.placeholder(tf.float32)\n",
    "        \n",
    "        # In the naive inception module, we have 6 layers: the input layer, followed by the 1x1 conv, 3x3 conv, 5x5 conv\n",
    "        # and 3x3 maxpooling layer and lastly the DepthConcat layer which I assume is a 1D\n",
    "        \n",
    "        patch_size = 1\n",
    "        one_by_one_conv_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, num_channels, feature_maps], stddev=initialised_weights_stddev))\n",
    "        one_by_one_conv_biases = tf.Variable(tf.constant(initialised_weights_stddev * 10, shape=[feature_maps]))\n",
    "        \n",
    "        patch_size = 3\n",
    "        three_by_three_conv_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, num_channels, feature_maps], stddev=initialised_weights_stddev))\n",
    "        three_by_three_conv_biases = tf.Variable(tf.constant(initialised_weights_stddev * 10, shape=[feature_maps]))\n",
    "        \n",
    "        patch_size = 5\n",
    "        five_by_five_conv_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, num_channels, feature_maps], stddev=initialised_weights_stddev))\n",
    "        five_by_five_conv_biases = tf.Variable(tf.constant(initialised_weights_stddev * 10, shape=[feature_maps]))\n",
    "        \n",
    "    \n",
    "        # The 3x3 maxpooling layer doesn't need any variables.\n",
    "               \n",
    "        \n",
    "        # Variables\n",
    "        layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, num_channels, feature_maps], stddev=initialised_weights_stddev))\n",
    "        layer1_biases = tf.Variable(tf.zeros([feature_maps]))\n",
    "\n",
    "        layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, feature_maps, feature_maps], stddev=initialised_weights_stddev))\n",
    "        layer2_biases = tf.Variable(tf.constant(initialised_weights_stddev * 10, shape=[feature_maps]))\n",
    "\n",
    "        conv_layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, feature_maps, feature_maps], stddev=initialised_weights_stddev))\n",
    "        conv_layer3_biases = tf.Variable(tf.constant(initialised_weights_stddev * 10, shape=[feature_maps]))\n",
    "        \n",
    "        conv_layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, feature_maps, feature_maps], stddev=initialised_weights_stddev))\n",
    "        conv_layer4_biases = tf.Variable(tf.constant(initialised_weights_stddev * 10, shape=[feature_maps]))\n",
    "        \n",
    "        conv_layer5_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, feature_maps, feature_maps], stddev=initialised_weights_stddev))\n",
    "        conv_layer5_biases = tf.Variable(tf.constant(initialised_weights_stddev * 10, shape=[feature_maps]))\n",
    "        \n",
    "        conv_layer6_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, feature_maps, feature_maps], stddev=initialised_weights_stddev))\n",
    "        conv_layer6_biases = tf.Variable(tf.constant(initialised_weights_stddev * 10, shape=[feature_maps]))\n",
    "\n",
    "        number_of_max_pool_layers = 3\n",
    "        conv_output_size = int(math.ceil(image_size / (2.0 ** number_of_max_pool_layers)) * math.ceil(image_size / (2.0 ** number_of_max_pool_layers)) * feature_maps)\n",
    "        #print \"conv_output_size %s\" % conv_output_size\n",
    "        layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "            [conv_output_size, number_of_hidden_neurons], stddev=initialised_weights_stddev))\n",
    "        layer3_biases = tf.Variable(tf.constant(initialised_weights_stddev * 10, shape=[number_of_hidden_neurons]))\n",
    "\n",
    "\n",
    "        layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "            [number_of_hidden_neurons, num_labels], stddev=initialised_weights_stddev))\n",
    "        layer4_biases = tf.Variable(tf.constant(initialised_weights_stddev * 10, shape=[num_labels]))\n",
    "\n",
    "        # Model.\n",
    "        def create_model_graph(data, add_dropout = False):\n",
    "            conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + layer1_biases)\n",
    "            #shape = hidden.get_shape().as_list()\n",
    "            #print \"hidden shape: %s\" % shape\n",
    "\n",
    "            conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            relu = tf.nn.relu(conv + layer2_biases)\n",
    "            hidden = tf.nn.max_pool(relu, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "            #shape = hidden.get_shape().as_list()\n",
    "            #print \"hidden shape: %s\" % shape\n",
    "\n",
    "            conv = tf.nn.conv2d(hidden, conv_layer3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + conv_layer3_biases)\n",
    "            \n",
    "            conv = tf.nn.conv2d(hidden, conv_layer4_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            relu = tf.nn.relu(conv + conv_layer4_biases)\n",
    "            hidden = tf.nn.max_pool(relu, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "            #shape = hidden.get_shape().as_list()\n",
    "            #print \"hidden shape: %s\" % shape\n",
    "            \n",
    "            conv = tf.nn.conv2d(hidden, conv_layer5_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + conv_layer5_biases)\n",
    "            \n",
    "            conv = tf.nn.conv2d(hidden, conv_layer6_weights, [1, 1, 1, 1], padding='SAME')\n",
    "            relu = tf.nn.relu(conv + conv_layer6_biases)\n",
    "            hidden = tf.nn.max_pool(relu, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "            #shape = hidden.get_shape().as_list()\n",
    "            #print \"hidden shape: %s\" % shape\n",
    "\n",
    "            shape = hidden.get_shape().as_list()\n",
    "            #print \"hidden shape: %s\" % shape\n",
    "            reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "            if add_dropout:\n",
    "                hidden = tf.nn.dropout(hidden, dropout_keep_probability)\n",
    "            return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "        # Training computation.\n",
    "        logits = create_model_graph(tf_train_dataset, add_dropout = True)\n",
    "        layer_weights = [layer1_weights, layer2_weights, conv_layer3_weights, conv_layer4_weights, conv_layer5_weights, conv_layer6_weights, layer3_weights, layer4_weights]\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) + get_l2_loss(l2_lambda, layer_weights))\n",
    "\n",
    "        # Optimizer.\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        decayed_learning_rate = tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(decayed_learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "        # Predictions for the training, validation, and test data.\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        valid_prediction = tf.nn.softmax(create_model_graph(tf_valid_dataset))\n",
    "        test_prediction = tf.nn.softmax(create_model_graph(tf_test_dataset))\n",
    "        \n",
    "        return Model(graph, batch_size, tf_train_dataset, tf_train_labels, tf_valid_dataset, tf_test_dataset, dropout_keep_probability, logits, loss, optimizer, train_prediction, valid_prediction, test_prediction)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colabVersion": "0.3.2",
  "colab_default_view": {},
  "colab_views": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
