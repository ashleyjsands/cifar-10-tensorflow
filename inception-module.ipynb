{
 "metadata": {
  "colabVersion": "0.3.2",
  "colab_default_view": {},
  "colab_views": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "name": "",
  "signature": "sha256:b8dde9217b00cd593271761619da2a69df11686aba78d6956ae6e808f637eef4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Attribution: Some of this code is from tensorflow tutorials and tensor udacity examples."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Goal\n",
      "====\n",
      "**My goal is to achieve 75.86% test accuracy** or higher for the CIFAR-10 dataset. My stretch goal is 90% test accuracy.\n",
      "\n",
      "My first naive adoption of the Not-MNIST conv net achieved ~26% after 5000 steps.\n",
      "\n",
      "For my first model, halving the decay step improved **validation accuracy to 69.08% and test accuracy to 68.77%**."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%autosave 60\n",
      "%load_ext autoreload\n",
      "%autoreload 2 # reload all packages\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "javascript": [
        "IPython.notebook.set_autosave_interval(6000)"
       ],
       "metadata": {},
       "output_type": "display_data"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Autosaving every 6 seconds\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from data import get_datasets_and_labels, reformat, Datasets, image_size, num_labels, num_channels\n",
      "from visual import visualise_accuracies\n",
      "from inception_module_model import create_inception_module_model\n",
      "from train import accuracy, train_model_in_batches, eval_predictions\n",
      "\n",
      "train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels = get_datasets_and_labels()\n",
      "\n",
      "print('Training set', train_dataset.shape, train_labels.shape)\n",
      "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
      "print('Test set', test_dataset.shape, test_labels.shape)\n",
      "\n",
      "print('\\nReformatting datasets')\n",
      "train_dataset, train_labels = reformat(train_dataset, train_labels, image_size, num_channels, num_labels)\n",
      "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels, image_size, num_channels, num_labels)\n",
      "test_dataset, test_labels = reformat(test_dataset, test_labels, image_size, num_channels, num_labels)\n",
      "\n",
      "datasets = Datasets(train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels)\n",
      "\n",
      "print('Training set', train_dataset.shape, train_labels.shape)\n",
      "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
      "print('Test set', test_dataset.shape, test_labels.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded cifar-10-python.tar.gz 170498071 bytes.\n",
        "Opening CIFAR 10 dataset\n",
        "Finished opening CIFAR 10 dataset"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training set"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (40000, 3072) (40000,)\n",
        "Validation set (10000, 3072) (10000,)\n",
        "Test set (10000, 3072) (10000,)\n",
        "\n",
        "Reformatting datasets\n",
        "Training set"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (40000, 32, 32, 3) (40000, 10)\n",
        "Validation set (10000, 32, 32, 3) (10000, 10)\n",
        "Test set (10000, 32, 32, 3) (10000, 10)\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 5K steps for GPU test\n",
      "model = create_inception_module_model(learning_rate = 0.00025, eval_batch_size=100, l2_lambda = 0.025, pre_layer_feature_maps = 128, feature_maps = 64, initialised_weights_stddev = 0.25, decay_steps = 5000, decay_rate = 0.96)\n",
      "steps_to_validation_predictions = train_model_in_batches(model, datasets, 5001, dropout_keep_prob = 0.9, load_model = False)\n",
      "correct_prediction_indexes, incorrect_prediction_indexes = visualise_accuracies(steps_to_validation_predictions, valid_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Initialized\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0 minibatch loss: 181.359 minibatch accuracy: 15.6% validation accuracy: 9.8%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000 minibatch loss: 132.395 minibatch accuracy: 12.5% validation accuracy: 14.8%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2000 minibatch loss: 130.323 minibatch accuracy: 25.0% validation accuracy: 23.3%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3000 minibatch loss: 128.703 minibatch accuracy: 9.4% validation accuracy: 24.5%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4000 minibatch loss: 127.213 minibatch accuracy: 37.5% validation accuracy: 25.5%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5000 minibatch loss: 125.736 minibatch accuracy: 21.9% validation accuracy: 27.8%\n",
        "Test accuracy at step 5000: 27.6%\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Elapsed time: 0.009060312045945061 hours\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEACAYAAABGYoqtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f5f060a85c0>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The best validation accuracy was 27.83 at step 5000\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the initial hyper parameters, for 100K steps, the best validation accuracy was 40%. The learning process was quite sporadic, which indicates bad hyper parameters or net architecture. Halving the learning rate improved the validation accuracy up to 42.6%. This is a good sign that lowering the learning rate may improve performance more and lead to less sporadic performance. Increasing the feature maps from 16 to 24 improve performance up to 43.7 which is a great sign but performance was still very sporadic.\n",
      "\n",
      "Lets try quartering the initial learning rate with 24 feature maps."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Half the initial learning rate, 24 feature maps and a decreased weight initialisation value\n",
      "model = create_naive_inception_module_model(learning_rate = 0.0005, l2_lambda = 0.1, feature_maps = 24, initialised_weights_stddev = 0.01, decay_steps = 10000, decay_rate = 0.96)\n",
      "steps_to_validation_predictions, steps_to_test_predictions = train_model_in_batches(model, 100001, dropout_keep_prob = 0.9)\n",
      "correct_prediction_indexes, incorrect_prediction_indexes = visualise_accuracies(steps_to_validation_predictions, steps_to_test_predictions)\n",
      "\n",
      "# Mac Elapsed time: 7.17482998166 hours\n",
      "# Linux CPU Elapsed time: 1.37846191777 hours\n",
      "# Speed up: 5.20\n",
      "\n",
      "# Linux GPU CUDA Compute 6.1 Elapsed time: 0.3188351786798901 hours\n",
      "# Speed up from Mac: 22.503257048\n",
      "# Speed up from Linux CPU: 4.32343107"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A weights std dev of 0.5 is better than 0.7."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Best values thus far: learning_rate = 0.0005, l2_lambda = 0.1, feature_maps = 24, initialised_weights_stddev = 0.5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Out of all of the experiements, decreasing the l2_lambda to 0.05 was the only one that improvement the validation accuracy:\n",
      "The best validation accuracy was 45.29 at step 96000\n",
      "The best test accuracy was 44.91 at step 90000"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Halve l2_lambda again\n",
      "model = create_inception_module_model(learning_rate = 0.00025, l2_lambda = 0.025, feature_maps = 24, initialised_weights_stddev = 0.5, decay_steps = 10000, decay_rate = 0.96)\n",
      "steps_to_validation_predictions, steps_to_test_predictions = train_model(model, 100001, dropout_keep_prob = 0.9)\n",
      "correct_prediction_indexes, incorrect_prediction_indexes = visualise_accuracies(steps_to_validation_predictions, steps_to_test_predictions)\n",
      "\n",
      "\"\"\"\n",
      "Minibatch loss at step 100000 : 3.98225\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy at step 100000: 45.4%\n",
      "Test accuracy at step 100000: 45.5%\n",
      "\n",
      "Elapsed time: 1.38459513499 hours\n",
      "The best validation accuracy was 45.87 at step 96000\n",
      "The best test accuracy was 45.55 at step 100000\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Best result:\n",
      "The best validation accuracy was 45.87 at step 96000\n",
      "The best test accuracy was 45.55 at step 100000"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is the best result thus far:\n",
      "The best validation accuracy was 47.81 at step 194000\n",
      "The best test accuracy was 47.23 at step 200000"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I'm currently facing the law of diminishing returns. This naive implementation of the inception module has its limitations. It is time to implement the full module."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 400K steps with a longer decay step of 50000\n",
      "model = create_naive_inception_module_model(learning_rate = 0.00025, l2_lambda = 0.025, feature_maps = 25, initialised_weights_stddev = 0.5, decay_steps = 50000, decay_rate = 0.96)\n",
      "steps_to_validation_predictions, steps_to_test_predictions = train_model(model, 400001, dropout_keep_prob = 0.9)\n",
      "correct_prediction_indexes, incorrect_prediction_indexes = visualise_accuracies(steps_to_validation_predictions, steps_to_test_predictions)\n",
      "\n",
      "***Best performance thus far:***\n",
      "\n",
      "Elapsed time: 6.38758515033 hours\n",
      "The best validation accuracy was 50.77 at step 377000\n",
      "The best test accuracy was 49.92 at step 370000"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The non naive inception module has a greater capacity to learn as its best performance was just before the end of the training with 400K steps and its the best record for the inception module thus far with a **validation accuracy of 54.1%**.\n",
      "\n",
      "The wise inception module had an 3.33% improvement over the naive inception module, but it also has a lot of room to grow.\n",
      "\n",
      "I should the wise inception module for longer and see how much it grows."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The best validation result was 54.1% at 400K steps. Giving the model another 400K steps, at 800K steps, the best validation performance was 57.6%. So doubling the number of steps improved performance by 3.5%. I would expect that this would abide by the law of diminishing returns and probably max out around 63-66%."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Running for another 1 Million increased validation performance from from 59.61% to 60.32%. A marginal improvement that still isn't good enough."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Changing the decay_steps slightly decreased validation set performance. This could be a negligible change."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Doubling the feature maps from 25 to 50 increased the training time form 8.8 hours to 11.39. This improved the best validation accuracy from 59.61 to 60.77 and the test accuracy from 58.2% to 59.9%. We are burded by the law of diminishing returns."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Adding a fully connected layer to the \"end\" of the network caused the network to stop learning because I added too many neurons (100). Reducing it to 32 neurons caused the network to learn again. Also, the learning rate was too high in combintation with the addition of a fully connected layer, that caused the neural network to stop learning.\n",
      "\n",
      "Adding 32 fully connected neurons reduced performance. Lets try a bit more."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The model with pre_layer_feature_maps = 250, feature_maps = 100 had an excellent validation accuracy of 63.34% at step 564K."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Increasing the pre_layer_feature_maps to 500 without increasing the feature maps actually reduced performance to 59.09%."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Increasing the decreasing the pre layer to inception module feature maps ratio improved performance, but not enough. So far the best ratio is 2.5."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Having a  pre layer to inception module feature maps ratio of 1 quickly caused the model to stop learning and revert. This has never happened before. The ratio of feature maps between layers is very important."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "The learning was very sporadic and jumpy. Could be a sign that the learning rate is too high.\n",
      "\n",
      "Best inception mode perfomance thus far: The best validation accuracy was 65.06 at step 809000."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Either because there were too many feature maps, or because the pre layer to module feature maps ratio was too small, this model did not learn."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's half the learning rate of the last successful model, and increase its run way to 1.5M steps."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Halving the learning rate did not work."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Success! This model has a validation accuracy of 66.26%! This is roughly equal to the performance of the previous model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's increase the number of pre_layer_feature_maps and increase steps to 1.5M.\n",
      "model = create_inception_module_model(learning_rate = 0.0001, l2_lambda = 0.025, pre_layer_feature_maps = 1500, feature_maps = 750, initialised_weights_stddev = 0.25, decay_steps = 50000, decay_rate = 0.96)\n",
      "steps_to_validation_predictions, steps_to_test_predictions = train_model_in_batches(model, 1500001, dropout_keep_prob = 0.9, load_model = False)\n",
      "correct_prediction_indexes, incorrect_prediction_indexes = visualise_accuracies(steps_to_validation_predictions, steps_to_test_predictions)\n",
      "\n",
      "\"\"\"\n",
      "step: 927000 minibatch loss: nan minibatch accuracy: 71.9% validation accuracy: 65.7%\n",
      "step: 928000 minibatch loss: nan minibatch accuracy: 71.9% validation accuracy: 64.8%\n",
      "step: 929000 minibatch loss: nan minibatch accuracy: 78.1% validation accuracy: 64.4%\n",
      "step: 930000 minibatch loss: nan minibatch accuracy: 78.1% validation accuracy: 64.6%\n",
      "step: 931000 minibatch loss: nan minibatch accuracy: 65.6% validation accuracy: 64.5%\n",
      "step: 932000 minibatch loss: nan minibatch accuracy: 68.8% validation accuracy: 64.4%\n",
      "step: 933000 minibatch loss: nan minibatch accuracy: 78.1% validation accuracy: 64.8%\n",
      "step: 934000 minibatch loss: nan minibatch accuracy: 75.0% validation accuracy: 65.5%\n",
      "step: 935000 minibatch loss: nan minibatch accuracy: 84.4% validation accuracy: 63.9%\n",
      "step: 936000 minibatch loss: nan minibatch accuracy: 65.6% validation accuracy: 65.9%\n",
      "step: 937000 minibatch loss: nan minibatch accuracy: 78.1% validation accuracy: 64.5%\n",
      "step: 938000 minibatch loss: nan minibatch accuracy: 75.0% validation accuracy: 64.4%\n",
      "step: 939000 minibatch loss: nan minibatch accuracy: 68.8% validation accuracy: 65.8%\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At step 760000, there was a **record breaking validation accuracy of 67.2%**. Increasing the prelayer feature maps does help."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's halve the initialised_weights_stddev with less steps of 750K.\n",
      "model = create_inception_module_model(learning_rate = 0.0001, l2_lambda = 0.025, pre_layer_feature_maps = 250, feature_maps = 125, initialised_weights_stddev = 0.125, decay_steps = 50000, decay_rate = 0.96)\n",
      "steps_to_validation_predictions, steps_to_test_predictions = train_model_in_batches(model,datasets, 50001, dropout_keep_prob = 0.9, load_model = False)\n",
      "correct_prediction_indexes, incorrect_prediction_indexes = visualise_accuracies(steps_to_validation_predictions, steps_to_test_predictions)\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Initialized\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0 minibatch loss: 135.474 minibatch accuracy: 12.5% validation accuracy: 9.5%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000 minibatch loss: 121.127 minibatch accuracy: 25.0% validation accuracy: 16.6%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2000 minibatch loss: 120.533 minibatch accuracy: 18.8% validation accuracy: 23.0%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3000 minibatch loss: 119.757 minibatch accuracy: 28.1% validation accuracy: 24.6%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4000 minibatch loss: 119.245 minibatch accuracy: 31.2% validation accuracy: 25.1%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5000 minibatch loss: 118.711 minibatch accuracy: 18.8% validation accuracy: 25.4%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6000 minibatch loss: 118.065 minibatch accuracy: 15.6% validation accuracy: 26.0%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7000 minibatch loss: 117.406 minibatch accuracy: 25.0% validation accuracy: 27.3%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8000 minibatch loss: 116.899 minibatch accuracy: 15.6% validation accuracy: 27.9%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 9000 minibatch loss: 116.338 minibatch accuracy: 31.2% validation accuracy: 27.3%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10000 minibatch loss: 115.725 minibatch accuracy: 25.0% validation accuracy: 28.1%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11000 minibatch loss: 115.012 minibatch accuracy: 43.8% validation accuracy: 28.2%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12000 minibatch loss: 114.49 minibatch accuracy: 37.5% validation accuracy: 29.6%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13000 minibatch loss: 113.905 minibatch accuracy: 28.1% validation accuracy: 29.8%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14000 minibatch loss: 113.561 minibatch accuracy: 21.9% validation accuracy: 30.4%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15000 minibatch loss: 112.894 minibatch accuracy: 21.9% validation accuracy: 30.5%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 16000 minibatch loss: 112.399 minibatch accuracy: 18.8% validation accuracy: 31.1%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 17000 minibatch loss: 111.665 minibatch accuracy: 34.4% validation accuracy: 32.2%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 18000 minibatch loss: 111.168 minibatch accuracy: 31.2% validation accuracy: 32.1%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 19000 minibatch loss: 110.802 minibatch accuracy: 31.2% validation accuracy: 32.6%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 20000 minibatch loss: 110.143 minibatch accuracy: 34.4% validation accuracy: 32.9%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 21000 minibatch loss: 109.594 minibatch accuracy: 40.6% validation accuracy: 32.4%\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-12-5f7fd8f155e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Let's halve the initialised_weights_stddev with less steps of 750K.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_inception_module_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml2_lambda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.025\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre_layer_feature_maps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_maps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m125\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitialised_weights_stddev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.125\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecay_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecay_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.96\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msteps_to_validation_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_to_test_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model_in_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mcorrect_prediction_indexes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincorrect_prediction_indexes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvisualise_accuracies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps_to_validation_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_to_test_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/ashley/code/cifar-10-tensorflow/train.py\u001b[0m in \u001b[0;36mtrain_model_in_batches\u001b[1;34m(model, datasets, steps, dropout_keep_prob, load_model)\u001b[0m\n\u001b[0;32m     37\u001b[0m             }\n\u001b[0;32m     38\u001b[0m             _, l, predictions = session.run(\n\u001b[1;32m---> 39\u001b[1;33m                 [model.optimizer, model.loss, model.train_prediction], feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m     40\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[0mtraining_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/ashley/.virtualenvs/tensorflow_py3_gpu_0.10/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    708\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 710\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    711\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/ashley/.virtualenvs/tensorflow_py3_gpu_0.10/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    906\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 908\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    909\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/ashley/.virtualenvs/tensorflow_py3_gpu_0.10/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 958\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    959\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
        "\u001b[1;32m/home/ashley/.virtualenvs/tensorflow_py3_gpu_0.10/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    963\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/ashley/.virtualenvs/tensorflow_py3_gpu_0.10/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    945\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m    946\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m    948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's halve the initialised_weights_stddev with less steps of 750K.\n",
      "model = create_inception_module_model(learning_rate = 0.0001, l2_lambda = 0.025, pre_layer_feature_maps = 1000, feature_maps = 500, initialised_weights_stddev = 0.125, decay_steps = 50000, decay_rate = 0.96)\n",
      "steps_to_validation_predictions, steps_to_test_predictions = train_model_in_batches(model,datasets, 800001, dropout_keep_prob = 0.9, load_model = False)\n",
      "correct_prediction_indexes, incorrect_prediction_indexes = visualise_accuracies(steps_to_validation_predictions, steps_to_test_predictions)\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Initialized\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0 minibatch loss: 1876.16 minibatch accuracy: 9.4% validation accuracy: 10.9%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000 minibatch loss: 1832.54 minibatch accuracy: 34.4% validation accuracy: 23.5%\n",
        "step:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2000 minibatch loss: 1824.25 minibatch accuracy: 21.9% validation accuracy: 26.5%\n",
        "step:"
       ]
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Todo list\n",
      "=========\n",
      "Check what the init values were several commits ago and see if those changes have caused the model to become unstable.\n",
      "\n",
      "Test more learning rates.\n",
      "\n",
      "Test the weight initialisation values.\n",
      "\n",
      "Look at the GoogleNet paper to see what changes I can make."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import subprocess\n",
      "subprocess.call([\"shutdown\", \"+2\"]) # iPython has been set to autosave every 1 minute and this will shutdown in 2 minutes."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO: stats at the end of training so that they can be resumed if needed."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}